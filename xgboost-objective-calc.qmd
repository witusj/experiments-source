---
title: "XGBoost regression model for objective calculation"
jupyter: python3
---

## Objective

*Compare the performance (speed and accuracy) of a surrogate model (XGBoost regressor) with a conventional calculation for appointment scheduling objective function and against a ranking model.*

## Background

*To find optimal solutions for appointment scheduling problems one approach is to create local search neighborhoods and evaluate the schedules in that set. A better search method either (1) - creates smaller search neighborhoods or (2) - evaluates faster.*

*One approach for speeding up evaluation is to create surrogate models, or metamodels. These are simplified representations of complex systems that are often created using machine learning techniques.*

*In an earlier experiment we developed a ranking model that can rank two schedules according to preference. We established that a ranking model is significantly faster at choosing the best solution from a pair while retaining high accuracy levels.*

*In this experiment we develop a Machine Learning model using XGBoost for evaluating a single schedule and let it compete with the conventional method as well as with the ranking model.*

## Hypothesis

*We expect a ranking model to be superior in speed compared to a XGBoost regressor model. The XGBoost regressor model will outperform the conventional model in speed.*

## Methodology

### Tools and Materials

List the software, libraries, datasets, and any other materials you will use in this experiment. Include specific versions or configurations that are crucial for reproducibility.

```{python}
import time
import math
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.base import clone
import xgboost as xgb
from xgboost.callback import TrainingCallback
import plotly.graph_objects as go
import pickle
```

### Experimental Design

*We will create a random set of pairs of neighboring schedules with* $N = 12$ patients and $\ T = 18$ intervals of length $d = 5$.

*A neighborhood consists of all schedules that differ by one patient only. Eg: (\[2,1,1\], \[1,1,2\]) are neighbors and (\[2,1,1\], \[1,0,3\]) are not.*

*Service times will have a discrete distribution. The probability of a scheduled patient not showing up will be* $q = 0.20$.

*The objective function will be the weighted average of the total waiting time of all patients and overtime. The model will be trained to predict the objective value of a given schedule. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value using the conventional method.*

```{python}
N = 12 # Number of patients
T = 18 # Number of intervals
d = 5 # Length of each interval
s = [0.0, 0.27, 0.28, 0.2, 0.15, 0.1] # Service times distribution
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.8 # Weight for the waiting time in objective function
num_schedules = 20000 # Number of schedules to sample
```

### Variables

-   **Independent Variables**: *A list schedules.*
-   **Dependent Variables**: *A list with objective values for each schedules.*

### Data Collection

*The data set has been generated in an earlier experiment using simulation in which random samples were drawn from the population of all possible schedules.*

```{python}
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
    data = pickle.load(f)

# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives_list = data['objectives']
rankings_list = data['rankings']

print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors, "\n")
for objectives in objectives_list[:2]: print(objectives, "\n")
for rankings in rankings_list[:2]: print(rankings, "\n")

```

### Sample Size and Selection

**Sample Size**: - Indicate the number of samples you will use in the experiment. Justify the size based on the needs of the experiment, such as ensuring statistical significance.

**Sample Selection**: - Describe how you will select your samples. Ensure that the sampling method is unbiased and representative of the population you are studying.

### Experimental Procedure

1.  *Train XGBoost regressor model to predict objective values from given schedules. Measure training time and get training accuracy.*

```{python}

X = np.array([X[0] for X in neighbors_list])
y = np.array([y[0] for y in objectives_list])

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#=========================================================================
# XGBoost regression: 
# Parameters: 
# n_estimators  "Number of gradient boosted trees. Equivalent to number 
#                of boosting rounds."
# learning_rate "Boosting learning rate (also known as “eta”)"
# max_depth     "Maximum depth of a tree. Increasing this value will make 
#                the model more complex and more likely to overfit." 
#=========================================================================
regressor=xgb.XGBRegressor(eval_metric='rmsle')

#=========================================================================
# exhaustively search for the optimal hyperparameters
#=========================================================================
from sklearn.model_selection import GridSearchCV
# set up our search grid
param_grid = {"max_depth":    [4, 5, 7],
              "n_estimators": [500, 700, 900],
              "learning_rate": [0.05, 0.1, 0.15]}

# try out every combination of the above values
start = time.time()
search = GridSearchCV(regressor, param_grid, cv=5, verbose=3).fit(X_train, y_train)
end = time.time()
hyper_search_time = end - start
print(f'Hyperparameter optimization time: {hyper_search_time}')

print("The best hyperparameters are ",search.best_params_)

regressor=xgb.XGBRegressor(learning_rate = search.best_params_["learning_rate"],
                       n_estimators  = search.best_params_["n_estimators"],
                       max_depth     = search.best_params_["max_depth"],
                       eval_metric='rmsle')

regressor.fit(X_train, y_train)
```

```{python}
predictions = regressor.predict(X_test)

from sklearn.metrics import mean_squared_log_error
RMSLE = np.sqrt( mean_squared_log_error(y_test, predictions) )
print("The score is %.5f" % RMSLE )
```

```{python}
import plotly.graph_objects as go

# Create the scatter plot
fig = go.Figure()

fig.add_trace(go.Scatter(
    x=y_test, 
    y=predictions, 
    mode='markers',
    marker=dict(color='blue'),
    name='Predictions vs. true values'
))
fig.add_trace(go.Scatter(
    x=[0, max(max(y_test), max(predictions))],
    y=[0, max(max(y_test), max(predictions))],
    mode='lines',
    line=dict(color='tomato', dash='dash'),
    name='Base line',
))

# Add axis labels and a title
fig.update_layout(
    title='Predictions vs. true values',
    xaxis_title='True values',
    yaxis_title='Predictions',
    showlegend=True
)

# Show the plot
fig.show()

```

2.  *Create validation set with pairs of neighboring schedules and calculate their objectives. Measure calculation time.*

```{python}
from functions import random_combination_with_replacement, create_neighbors_list, calculate_objective

num_test_schedules = 1000

test_schedules = random_combination_with_replacement(T, N, num_test_schedules)
test_neighbors = create_neighbors_list(test_schedules)

print(f"Sampled: {len(test_schedules)} schedules\n")

test_objectives_schedule_1 = [w * calculate_objective(test_neighbor[0], s, d, q)[0] + (1 - w) * calculate_objective(test_neighbor[0], s, d, q)[1] for test_neighbor in test_neighbors]
# Start time measeurement for the evaluation
start = time.time()
test_objectives_schedule_2 = [w * calculate_objective(test_neighbor[1], s, d, q)[0] + (1 - w) * calculate_objective(test_neighbor[1], s, d, q)[1] for test_neighbor in test_neighbors]
test_rankings = [0 if test_obj < test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]
end = time.time()
evaluation_time = end - start

# Combine the objectives for each pair for later processing
test_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]

print(f"\nEvaluation time: {evaluation_time} seconds\n")

for i in range(6):
    print(f"Neighbors: {test_neighbors[i]},\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\n")
```

2.  *Predict for each schedule in the validation set the objectives using the regressor model. Measure prediction time.*

```{python}

def predict_and_rank(neighbors):
    neighbors_array = [np.array(neighbor) for neighbor in neighbors] # Convert schedules to a NumPy array
    neighbors_array = np.vstack(neighbors_array)
    predictions = regressor.predict(neighbors_array)
    ranking = np.argmin(predictions)
    return predictions, ranking

predictions = [predict_and_rank(neighbors)[0] for neighbors in test_neighbors]
pred_rankings = [predict_and_rank(neighbors)[1] for neighbors in test_neighbors]
print(predictions[:10], pred_rankings[:10])
```

3.  *Calculate accuracy comparing true and predicted rankings.*
```{python}
errors = np.abs(np.array(test_rankings) - pred_rankings)
accuracy = 1 - errors.mean()
print(f"Accuracy = {accuracy}")

def calculate_ambiguousness(vector: np.array) -> float:
    # Ensure the vector is a numpy array
    vector = np.array(vector)
    
    # Calculate the angle of the vector in radians
    angle = np.arctan2(vector[1], vector[0])
    
    # Convert the angle to degrees
    angle_degrees = np.degrees(angle)
    
    # Calculate the absolute difference from 45 degrees
    difference_from_45 = np.abs(angle_degrees - 45)
    
    # Normalize the difference to a value between 0 and 1
    ambiguousness = difference_from_45 / 45
    
    # Since ambiguousness must be between 0 and 1, we cap it at 1
    ambiguousness = min(ambiguousness, 1)
    
    return ambiguousness

# Example usage
vector = np.array([1, 1])
ambiguousness = [calculate_ambiguousness(vector) for vector in predictions]
```


```{python}
df = pd.DataFrame({"Ambiguousness": ambiguousness, "Error": errors}).sort_values(by="Ambiguousness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()


# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Error"],
                    mode="markers",
                    name="Error",
                    marker=dict(size=9)))
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Cumulative accuracy"],
                    mode="lines",
                    name="Cum. accuracy",
                    line = dict(width = 3, dash = 'dash')))
fig.update_layout(
    title={
        'text': f"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>",
        'y': 0.95,  # Keep the title slightly higher
        'x': 0.02,
        'xanchor': 'left',
        'yanchor': 'top'
    },
    xaxis_title="Ambiguousness",
    yaxis_title="Error / Accuracy",
    hoverlabel=dict(font=dict(color='white')),
    margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
```


## Results

Present your findings, using visual aids like charts or tables where appropriate. This section is factual; simply report what you found during the experiment.

## Discussion

Analyze your results in this section. Discuss whether your hypothesis was supported, what the results mean, and the implications for future work. Address any anomalies or unexpected findings, and consider the broader impact of your results.

## Timeline

**This experiment was started on 30-08-2024. The expected completion date is 09-09-2024.**

## References

Cite all sources that informed your experiment, including research papers, datasets, and tools. This section ensures that your work is properly grounded in existing research and that others can trace the origins of your methods and data.
