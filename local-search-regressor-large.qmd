---
title: "Large instance local search with trained XGBoost regressor model"
jupyter: python3
---

## Objective

Test the working and performance of a [previously trained](xgboost-pairwise-ranking.qmd) XGBoost Regressor model in a local search application.

## Background

In previous experiments, we trained an XGBoost Regressor model to predict the objective values of neighboring schedules. In this experiment, we will use the trained models to perform a local search to find the best schedule.

## Hypothesis

The XGBoost Regressor model will be able to efficiently guide the local search algorithm to find a schedule with a lower objective value than the initial schedule.

## Methodology

### Tools and Materials

```{python}
import numpy as np
import json
from itertools import chain, combinations
import sys
from math import comb  # Available in Python 3.8 and later
import xgboost as xgb
from functions import calculate_objective
import pickle
```

### Load Parameters

```{python}
N = 22
T = 20
l = 10

file_path_parameters = f"datasets/parameters_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_parameters, 'rb') as f:
    data_params = pickle.load(f)

for key in data_params.keys():
  print(f"{key} = {data_params[key]}")

N = data_params['N'] # Number of patients
T = data_params['T'] # Number of intervals
d = data_params['d'] # Length of each interval
max_s = data_params['max_s'] # Maximum service time
q = data_params['q'] # Probability of a scheduled patient not showing up
w = data_params['w'] # Weight for the waiting time in objective function
l = data_params['l']

for key in data_params.keys():
  print(f"{key} = {data_params[key]}")
  
num_schedules = data_params['num_schedules'] # Number of schedules to sample
convolutions = data_params['convolutions']
```

### Experimental Design

We will use the trained XGBoost Regressor model to guide a local search algorithm to find the best schedule. The local search algorithm will start with an initial schedule and iteratively explore the neighborhood of the current schedule to find a better one. As an initial schedule, we will use the schedule with the lowest objective value from the training dataset that was used to train the XGBoost Regressor model.

### Variables

-   **Independent Variables**:
    -   The trained XGBoost Regressor model.
-   **Dependent Variables**:
    -   Speed, accuracy, and convergence of the local search algorithm.

### Data Collection

We will use the training dataset to initialize the local search algorithm.

### Sample Size and Selection

### Experimental Procedure

![Local search algorithm](images/local_search_algorithm.png){#fig-local-search-algorithm}

## Results

### Load the initial best schedule.

Start with the best solution found so far $\{x^*, C(x^*)\}$ from the training set.

```{python}
# Load the best solution from the training dataset
file_path_schedules = f"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_schedules, 'rb') as f:
    data_sch = pickle.load(f)
    
print(f"The data has following keys: {[key for key in data_sch.keys()]}")

# Step 1: Flatten the objectives into a 1D array
flattened_data = [value for sublist in data_sch['objectives'] for value in sublist]

# Step 2: Find the index of the minimum value
min_index = np.argmin(flattened_data)

# Step 3: Convert that index back to the original 2D structure
row_index = min_index // 2  # Assuming each inner list has 2 values
col_index = min_index % 2

print(f"The minimum objective value is at index [{row_index}][{col_index}].\nThis is schedule: {data_sch['neighbors_list'][row_index][col_index]} with objective value {data_sch['objectives'][row_index][col_index]}.")

# Set the initial schedule to the best solution from the training dataset
initial_schedule = data_sch['neighbors_list'][row_index][col_index]
N = sum(initial_schedule)
T = len(initial_schedule)
```

### Generate the neighborhood of $x^*$.

#### Set T

Set $T$ to the length of the vector $x^*$.

```{python}
T = len(initial_schedule)
```

#### Define $V^*$.

Define the vectors $V^*$ as follows:

$$
\left\{
\begin{array}{c}
\vec{v_1}, \\
\vec{v_2}, \\
\vec{v_3}, \\
\vdots \\
\vec{v_{T-1}}, \\
\vec{v_T} \\
\end{array}
\right\} = 
\left\{
\begin{array}{c}
(-1, 0,...., 0, 1), \\
(1, -1, 0,...., 0), \\
(0, 1, -1,...., 0), \\
\vdots \\
(0,...., 1, -1, 0), \\
(0,...., 0, 1, -1) \\
\end{array}
\right\}
$$

```{python}
def get_v_star(t):
    # Create an initial vector 'u' of zeros with length 't'
    u = np.zeros(t, dtype=int)
    # Set the first element of vector 'u' to -1
    u[0] = -1
    # Set the last element of vector 'u' to 1
    u[-1] = 1
    # Initialize the list 'v_star' with the initial vector 'u'
    v_star = [u]
    # Loop over the length of 'u' minus one times
    for i in range(len(u) - 1):
        # Append the last element of 'u' to the front of 'u'
        u = np.append(u[-1], u)
        # Remove the last element of 'u' to maintain the same length
        u = np.delete(u, -1)
        # Append the updated vector 'u' to the list 'v_star'
        v_star.append(u)
    # Convert the list of vectors 'v_star' into a NumPy array and return it
    return(np.array(v_star))

# Example of function call:
# This will create a 4x4 matrix where each row is a cyclically shifted version of the first row
get_v_star(4)
```

#### Define $U_t$.

Define $U_t$ as the set of all possible subsets of $V^*$ such that each subset contains exactly $t$ elements, i.e.,

$$
U_t = \{ S \subsetneq V^* \mid |S| = t \}, \quad t \in \{1, 2, \dots, T\}.
$$

```{python}
def powerset(iterable, size=1):
    "powerset([1,2,3], 2) --> (1,2) (1,3) (2,3)"
    return [[i for i in item] for item in combinations(iterable, size)]
  
x = initial_schedule

# Generate a matrix 'v_star' using the 'get_v_star' function
v_star = get_v_star(T)

# Generate all possible non-empty subsets (powerset) of the set {0, 1, 2, ..., t-1}
# 'ids' will be a list of tuples, where each tuple is a subset of indices
size = 2
ids = powerset(range(T), size)
len(ids)
ids[:T]
```

#### Define the neighborhood of $x$

Define the neighborhood of $x$ as all vectors of the form $x + u_{tk}$ with $\forall \, u_{tk} \in U_t$.

```{python}
v_star = get_v_star(T)

def get_neighborhood(x, v_star, ids, verbose=False):
    x = np.array(x)
    p = 50
    if verbose:
        print(f"Printing every {p}th result")
    # Initialize the list 'neighborhood' to store the vectors in the neighborhood of 'x'
    neighborhood = []
    # Loop over all possible non-empty subsets of indices
    for i in range(len(ids)):
        # Initialize the vector 'neighbor' to store the sum of vectors in 'v_star' corresponding to the indices in 'ids[i]'
        neighbor = np.zeros(len(x), dtype=int)
        # Loop over all indices in 'ids[i]'
        for j in range(len(ids[i])):
            if verbose:
                print(f"v_star{[ids[i][j]]}: {v_star[ids[i][j]]}")
            # Add the vector in 'v_star' corresponding to the index 'ids[i][j]' to 'neighbor'
            neighbor += v_star[ids[i][j]]
        # Append the vector 'x' plus 'neighbor' to the list 'neighborhood'
        x_n = x + neighbor
        if i%p==0:
            if verbose:
                print(f"x, x', delta:\n{x},\n{x_n},\n{neighbor}\n----------------- ")
        neighborhood.append(x_n)
    
    # Convert the list 'neighborhood' into a NumPy array
    neighborhood = np.array(neighborhood)
    if verbose:
        print(f"Size of raw neighborhood: {len(neighborhood)}")
    # Create a mask for rows with negative values
    mask = ~np.any(neighborhood < 0, axis=1)
    # Filter out rows with negative values using the mask
    if verbose:
        print(f"filtered out: {len(neighborhood)-mask.sum()} schedules with negative values.")
    filtered_neighborhood = neighborhood[mask]
    if verbose:
        print(f"Size of filtered neighborhood: {len(filtered_neighborhood)}")
    return filtered_neighborhood

# Example of function call:
# This will generate the neighborhood of the vector 'x' using the vectors in 'v_star' and the indices in 'ids'
test_nh = get_neighborhood(x, v_star, ids)
print(f"All neighborhoods with {size} patients switched:\n x = {np.array(x)}: \n {test_nh}")
```

### Local search algorithm

1.  Generate the neighborhood of $x^*$.
2.  For each vector $y$ in the neighborhood of $x^*$:
    i.  Predict $C(y)$.
    ii. If $C(y) < C(x^*)$, set $x^* = y$ and go to 1
3.  Return $x^*$.

```{python}

from functions import calculate_objective_serv_time_lookup

def local_search_predicted(x, d, q, convolutions, w, v_star, regressor, size=2):
    # Ensure x_star is a 1D numpy array
    x_star = np.array(x).flatten()
    c_star = regressor.predict(x_star.reshape(1, -1))[0]

    solutions_list = []
    predictions_list = []
    objectives_list = []

    # Set T as the length of x_star
    T = len(x_star)

    # Outer loop for t (number of patients switched)
    t = 1
    while t < size:
        print(f'Running local search {t}')
        
        # Generate neighborhood and use a generator to reduce memory usage
        ids_gen = powerset(range(T), t)
        neighborhood = get_neighborhood(x_star, v_star, ids_gen)
        
        # Collect neighbors
        neighbors = []
        for neighbor in neighborhood:
            neighbors.append(neighbor)

        neighbors_array = np.array(neighbors)
        predicted_costs = regressor.predict(neighbors_array)

        # Flag to track if we find a better solution
        found_better_solution = False
        
        # Evaluate neighbors and update x_star and c_star
        for i, (neighbor, cost) in enumerate(zip(neighbors, predicted_costs)):
            if cost < c_star:
                x_star = neighbor
                c_star = cost
                objectives = calculate_objective_serv_time_lookup(x_star, d, q, convolutions)
                objective_value = w * objectives[0] + (1 - w) * objectives[1]
                print(f"Found better solution: {x_star}, pred_cost: {c_star}, real_cost: {objective_value}")
                
                # Update lists with the new best solution
                solutions_list.append(x_star)
                predictions_list.append(c_star)
                objectives_list.append(objective_value)
                
                # Increase n_estimators
                regressor.n_estimators += 5
                # Fit with previous model
                print(f"Retraining on {len(solutions_list)} new schedules")
                regressor.fit(
                    np.array(solutions_list),
                    np.array(objectives_list),
                    verbose=False
                )

                # Set flag to True and break out of inner loop
                found_better_solution = True
                break

        # If we found a better solution, restart outer loop from t = 1
        if found_better_solution:
            t = 1  # Restart search with t = 1
        else:
            t += 1  # Move to next t value if no better solution was found

    # Return the best solution found
    return x_star, c_star, objective_value, solutions_list, predictions_list, objectives_list

```

```{python}

def local_search(x, d, q, convolutions, w, v_star, size=2):
    # Initialize the best solution found so far 'x_star' to the input vector 'x'
    x_star = np.array(x).flatten()  # Keep as 1D array

    # Calculate initial objectives and cost
    objectives_star = calculate_objective_serv_time_lookup(x_star, d, q, convolutions)
    c_star = w * objectives_star[0] + (1 - w) * objectives_star[1]

    # Set the value of 'T' to the length of the input vector 'x'
    T = len(x_star)

    # Outer loop for the number of patients to switch
    t = 1
    while t < size:
        print(f'Running local search {t}')

        # Generate the neighborhood of the current best solution 'x_star' with 't' patients switched
        ids_gen = powerset(range(T), t)
        neighborhood = get_neighborhood(x_star, v_star, ids_gen)
        print(f"Switching {t} patient(s). Size of neighborhood: {len(list(ids_gen))}")

        # Flag to track if a better solution is found
        found_better_solution = False

        for neighbor in neighborhood:
            # Calculate objectives for the neighbor
            objectives = calculate_objective_serv_time_lookup(neighbor, d, q, convolutions)
            cost = w * objectives[0] + (1 - w) * objectives[1]

            # Compare scalar costs
            if cost < c_star:
                x_star = neighbor
                c_star = cost
                print(f"Found better solution: {x_star}, cost: {c_star}")

                # Set the flag to restart the outer loop
                found_better_solution = True
                break  # Break out of the inner loop

        # If a better solution was found, restart the search from t = 1
        if found_better_solution:
            t = 1  # Restart search with t = 1
        else:
            t += 1  # Move to the next neighborhood size if no better solution was found

    # Return the best solution found 'x_star' and its cost
    return x_star, c_star

```

### Run the local search algorithm

```{python}
# Example of using the local search algorithm with a regressor model
# Load regressor model
# Load parameters
with open('models/regressor_params.json', 'r') as f:
    params = json.load(f)
regressor = xgb.XGBRegressor(**params)
regressor.load_model("models/regressor_large_instance.json")
# Check parameters
print(regressor.get_params())

test = local_search_predicted(initial_schedule, d, q, convolutions, w, v_star , regressor, T)
print(test[:3])
print(f"Best solution found: {test[0]}, with predicted cost: {test[1]} and real cost: {test[2]}")
```

```{python}
import plotly.graph_objects as go

# Data for plotting
steps = list(range(len(test[4])))  # Convert range to list
predicted_values = test[4]
objective_values = test[5]

# Create the figure
fig = go.Figure()

# Add traces
fig.add_trace(go.Scatter(
    x=steps, 
    y=predicted_values,
    mode='lines',
    name='Predicted Objective Value',
))

fig.add_trace(go.Scatter(
    x=steps, 
    y=objective_values,
    mode='lines',
    name='True Objective Value',
    marker=dict(size=6, symbol='circle')
))

# Add titles and labels
fig.update_layout(
    title='Cost of Best Solution at Each Iteration',
    xaxis_title='Iteration',
    yaxis_title='Cost',
    legend_title='Cost Type',
    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
    xaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),
    yaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),
)

# Show the plot
fig.show()
fig.write_html("images/objectives-large-comparison.html")

```

```{python}
# Computing optimal solution with real cost
print(f"Initial schedule: {test[0]}")
test_x = local_search(test[0], d, q, convolutions, w, v_star, T)
test_x_pred = np.array(test_x[0]).flatten()  # Keep as 1D array
test_c_star_pred = regressor.predict(test_x_pred.reshape(1, -1))[0]
print(f"Best solution found: {test_x [0]}, with true cost: {test_x [1]}, and predicted cost: {test_c_star_pred}")
```

## Discussion

Analyze your results in this section. Discuss whether your hypothesis was supported, what the results mean, and the implications for future work. Address any anomalies or unexpected findings, and consider the broader impact of your results.

## Timeline

Document the duration and key dates of the experiment. This helps in project management and reproducibility.

## References

Cite all sources that informed your experiment, including research papers, datasets, and tools. This section ensures that your work is properly grounded in existing research and that others can trace the origins of your methods and data.s
