yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
from functions import compare_json
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
differences = compare_json(model_params, best_trial_params)
params_tbl = pd.DataFrame(differences)
params_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)
print(params_tbl)
# Fit the model on the entire dataset
# Initialize the XGBClassifier without early stopping here
# Load the best trial parameters from a JSON file.
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
start = time.time()
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=best_trial_params["max_depth"],
min_child_weight=best_trial_params["min_child_weight"],
gamma=best_trial_params["gamma"],
subsample=best_trial_params["subsample"],
colsample_bytree=best_trial_params["colsample_bytree"],
learning_rate=best_trial_params["learning_rate"],
n_estimators=best_trial_params["n_estimators"],
)
clf.fit(X, y)
end= time.time()
modeling_time = end - start
print(f"\nTraining time: {modeling_time} seconds\n")
# Calculate and print the training accuracy
training_accuracy = clf.score(X, y)
print(f"Training accuracy: {training_accuracy * 100:.2f}%")
# Predict the target for new data
y_pred = clf.predict(X_new)
# Probability estimates
start = time.time()
y_pred_proba = clf.predict_proba(X_new)
end = time.time()
prediction_time = end - start
print(f"\nPrediction time: {prediction_time} seconds\n")
print(f"test_rankings = {np.array(test_rankings)[:6]}, \ny_pred = {y_pred[:6]}, \ny_pred_proba = \n{y_pred_proba[:6]}")
errors = np.abs(y_pred - np.array(test_rankings))
ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)
df = pd.DataFrame({"Ambiguousness": ambiguousness, "Error": errors}).sort_values(by="Ambiguousness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9)))
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Ambiguousness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
import numpy as np
import plotly.graph_objects as go
training_time = round(modeling_time, 4)
conventional_time = round(evaluation_time, 4)
xgboost_time = round(prediction_time, 4)
# Define time values for plotting
time_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds
# Calculate evaluations for method 1
method1_evaluations = np.where(time_values >= training_time, (time_values - training_time) / xgboost_time * 1000, 0)
# Calculate evaluations for method 2
method2_evaluations = time_values / conventional_time * 1000
# Create line chart
fig = go.Figure()
# Add method 1 trace
fig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Surrogate model (XGBoost)'))
# Add method 2 trace
fig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))
# Update layout
fig.update_layout(
title="Number of Evaluations vs Time",
xaxis_title="Time (seconds)",
yaxis_title="Number of Evaluations",
legend_title="Methods",
template="plotly_white"
)
fig.show()
import time
import math
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.base import clone
import xgboost as xgb
from xgboost.callback import TrainingCallback
import plotly.graph_objects as go
N = 12
T = 18
d = 5 # Length of each interval
s = [0.0, 0.27, 0.28, 0.2, 0.15, 0.1] # Service times distribution
q = 0.20 # Probability of a scheduled patient not showing up
w = 1.0 # Weight for the waiting time in objective function
num_schedules = 20000
from functions import random_combination_with_replacement
start = time.time()
schedules = random_combination_with_replacement(T, N, num_schedules)
print(f"Sampled: {len(schedules)} schedules\n")
for schedule in schedules[:5]:
print(f"Schedule: {schedule}")
end = time.time()
data_prep_time = end - start
print(f"\nProcessing time: {data_prep_time} seconds\n")
from functions import create_neighbors_list
start = time.time()
neighbors_list = create_neighbors_list(schedules)
for neighbor in neighbors_list[:5]:
print(f"Neighbor: {neighbor[1]}")
end = time.time()
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import calculate_objective
objectives_schedule_1 = [w * calculate_objective(neighbor[0], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[0], s, d, q)[1]for neighbor in neighbors_list]
start = time.time()
objectives_schedule_2 = [w * calculate_objective(neighbor[1], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[1] for neighbor in neighbors_list]
end = time.time()
training_set_lab_time = end - start
objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]
# rankings = [0 if obj[0] < obj[1] else 1 for obj in objectives]
rankings = np.argmin(objectives, axis=1).tolist()
for i in range(5):
print(f"Objectives: {objectives[i]}, Ranking: {rankings[i]}")
print(f"\nProcessing time: {training_set_lab_time} seconds\n")
# Prepare the dataset
X = []
for neighbors in neighbors_list:
X.append(neighbors[0] + neighbors[1])
X = np.array(X)
y = np.array(rankings)
# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
class CustomCallback(TrainingCallback):
def __init__(self, period=10):
self.period = period
def after_iteration(self, model, epoch, evals_log):
if (epoch + 1) % self.period == 0:
print(f"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}")
return False
def fit_and_score(estimator, X_train, X_test, y_train, y_test):
"""Fit the estimator on the train set and score it on both sets"""
estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0
)
train_score = estimator.score(X_train, y_train)
test_score = estimator.score(X_test, y_test)
return estimator, train_score, test_score
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)
# Initialize the XGBClassifier without early stopping here
# Load the best trial parameters from a JSON file.
with open("model_params.json", "r") as f:
model_params = json.load(f)
# Initialize the EarlyStopping callback with validation dataset
early_stop = xgb.callback.EarlyStopping(
rounds=10, metric_name='logloss', data_name='validation_0', save_best=True
)
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=model_params["max_depth"],
min_child_weight=model_params["min_child_weight"],
gamma=model_params["gamma"],
subsample=model_params["subsample"],
colsample_bytree=model_params["colsample_bytree"],
learning_rate=model_params["learning_rate"],
n_estimators=model_params["n_estimators"],
early_stopping_rounds=9,
#callbacks=[CustomCallback(period=50), early_stop],
callbacks=[CustomCallback(period=50)],
)
print("Params: ")
for key, value in model_params.items():
print(f" {key}: {value}")
start = time.time()
results = []
for train_idx, test_idx in cv.split(X, y):
X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]
est, train_score, test_score = fit_and_score(
clone(clf), X_train, X_test, y_train, y_test
)
results.append((est, train_score, test_score))
end = time.time()
training_time = end - start
print(f"\nTraining time: {training_time} seconds\n")
# Print results
for i, (est, train_score, test_score) in enumerate(results):
print(f"Fold {i+1} - Train Score: {train_score:.4f}, Test Score: {test_score:.4f}")
# Fit the model on the entire dataset
# Initialize the XGBClassifier without early stopping here
start = time.time()
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=model_params["max_depth"],
min_child_weight=model_params["min_child_weight"],
gamma=model_params["gamma"],
subsample=model_params["subsample"],
colsample_bytree=model_params["colsample_bytree"],
learning_rate=model_params["learning_rate"],
n_estimators=model_params["n_estimators"],
)
clf.fit(X, y)
end= time.time()
modeling_time = end - start
# Calculate and print the training accuracy
training_accuracy = clf.score(X, y)
print(f"Training accuracy: {training_accuracy * 100:.2f}%\n")
print(f"\nTraining time: {modeling_time} seconds\n")
num_test_schedules = 1000
test_schedules = random_combination_with_replacement(T, N, num_test_schedules)
test_neighbors = create_neighbors_list(test_schedules)
print(f"Sampled: {len(test_schedules)} schedules\n")
test_objectives_schedule_1 = [calculate_objective(test_neighbor[0], s, d, q)[0] for test_neighbor in test_neighbors]
# Start time measeurement for the evaluation
start = time.time()
test_objectives_schedule_2 = [calculate_objective(test_neighbor[1], s, d, q)[0] for test_neighbor in test_neighbors]
test_rankings = [0 if test_obj < test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]
end = time.time()
evaluation_time = end - start
# Combine the objectives for each pair for later processing
test_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]
print(f"\nEvaluation time: {evaluation_time} seconds\n")
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\n")
input_X = test_neighbors
X_new = []
for test_neighbor in input_X:
X_new.append(test_neighbor[0] + test_neighbor[1])
# Predict the target for new data
y_pred = clf.predict(X_new)
# Probability estimates
start = time.time()
y_pred_proba = clf.predict_proba(X_new)
end = time.time()
prediction_time = end - start
print(f"\nPrediction time: {prediction_time} seconds\n")
print(f"test_rankings = {np.array(test_rankings)[:6]}, \ny_pred = {y_pred[:6]}, \ny_pred_proba = \n{y_pred_proba[:6]}")
from functions import calculate_ambiguousness
errors = np.abs(y_pred - np.array(test_rankings))
ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)
df = pd.DataFrame({"Ambiguousness": ambiguousness, "Error": errors}).sort_values(by="Ambiguousness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9)))
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Ambiguousness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
from functions import compare_json
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
differences = compare_json(model_params, best_trial_params)
params_tbl = pd.DataFrame(differences)
params_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)
print(params_tbl)
# Fit the model on the entire dataset
# Initialize the XGBClassifier without early stopping here
# Load the best trial parameters from a JSON file.
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
start = time.time()
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=best_trial_params["max_depth"],
min_child_weight=best_trial_params["min_child_weight"],
gamma=best_trial_params["gamma"],
subsample=best_trial_params["subsample"],
colsample_bytree=best_trial_params["colsample_bytree"],
learning_rate=best_trial_params["learning_rate"],
n_estimators=best_trial_params["n_estimators"],
)
clf.fit(X, y)
end= time.time()
modeling_time = end - start
print(f"\nTraining time: {modeling_time} seconds\n")
# Calculate and print the training accuracy
training_accuracy = clf.score(X, y)
print(f"Training accuracy: {training_accuracy * 100:.2f}%")
# Predict the target for new data
y_pred = clf.predict(X_new)
# Probability estimates
start = time.time()
y_pred_proba = clf.predict_proba(X_new)
end = time.time()
prediction_time = end - start
print(f"\nPrediction time: {prediction_time} seconds\n")
print(f"test_rankings = {np.array(test_rankings)[:6]}, \ny_pred = {y_pred[:6]}, \ny_pred_proba = \n{y_pred_proba[:6]}")
errors = np.abs(y_pred - np.array(test_rankings))
ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)
df = pd.DataFrame({"Ambiguousness": ambiguousness, "Error": errors}).sort_values(by="Ambiguousness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9)))
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Ambiguousness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
import numpy as np
import plotly.graph_objects as go
training_time = round(modeling_time, 4)
conventional_time = round(evaluation_time, 4)
xgboost_time = round(prediction_time, 4)
# Define time values for plotting
time_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds
# Calculate evaluations for method 1
method1_evaluations = np.where(time_values >= training_time, (time_values - training_time) / xgboost_time * 1000, 0)
# Calculate evaluations for method 2
method2_evaluations = time_values / conventional_time * 1000
# Create line chart
fig = go.Figure()
# Add method 1 trace
fig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))
# Add method 2 trace
fig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))
# Update layout
fig.update_layout(
title="Speed comparison between XGBoost ranking model and conventional method",
xaxis_title="Time (seconds)",
yaxis_title="Number of Evaluations",
legend_title="Methods",
template="plotly_white"
)
fig.show()
from functions import calculate_objective
objectives_schedule_1 = [w * calculate_objective(neighbor[0], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[0], s, d, q)[1] for neighbor in neighbors_list]
start = time.time()
objectives_schedule_2 = [w * calculate_objective(neighbor[1], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[1] for neighbor in neighbors_list]
end = time.time()
training_set_lab_time = end - start
objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]
# rankings = [0 if obj[0] < obj[1] else 1 for obj in objectives]
rankings = np.argmin(objectives, axis=1).tolist()
for i in range(5):
print(f"Objectives: {objectives[i]}, Ranking: {rankings[i]}")
print(f"\nProcessing time: {training_set_lab_time} seconds\n")
from functions import calculate_objective
objectives_schedule_1 = [w * calculate_objective(neighbor[0], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[0], s, d, q)[1] for neighbor in neighbors_list]
start = time.time()
objectives_schedule_2 = [w * calculate_objective(neighbor[1], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[1], s, d, q)[1] for neighbor in neighbors_list]
end = time.time()
training_set_lab_time = end - start
objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]
# rankings = [0 if obj[0] < obj[1] else 1 for obj in objectives]
rankings = np.argmin(objectives, axis=1).tolist()
for i in range(5):
print(f"Objectives: {objectives[i]}, Ranking: {rankings[i]}")
print(f"\nProcessing time: {training_set_lab_time} seconds\n")
import time
import math
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.base import clone
import xgboost as xgb
from xgboost.callback import TrainingCallback
import plotly.graph_objects as go
import pickle
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives = data['objectives']
print("Data loaded successfully.")
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives = data['objectives']
print("Data loaded successfully.\n")
neighbors_list[:5]
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives = data['objectives']
print("Data loaded successfully.\n")
neighbors_list[:2]
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives = data['objectives']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2] print(neigbors)
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives = data['objectives']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors)
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives = data['objectives']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors, \n)
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives = data['objectives']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors, "\n")
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives_list = data['objectives']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors, "\n")
for objectives in objectives_list[:2]: print(objectives, "\n")
from functions import calculate_objective
objectives_schedule_1 = [w * calculate_objective(neighbor[0], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[0], s, d, q)[1] for neighbor in neighbors_list]
start = time.time()
objectives_schedule_2 = [w * calculate_objective(neighbor[1], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[1], s, d, q)[1] for neighbor in neighbors_list]
end = time.time()
training_set_lab_time = end - start
objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]
rankings = np.argmin(objectives, axis=1).tolist()
for i in range(5):
print(f"Objectives: {objectives[i]}, Ranking: {rankings[i]}")
print(f"\nProcessing time: {training_set_lab_time} seconds\n")
# Saving neighbors_list and objectives to a pickle file
with open('neighbors_and_objectives.pkl', 'wb') as f:
pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)
print("Data saved successfully to 'neighbors_and_objectives.pkl'")
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives_list = data['objectives']
rankings_list = data['rankings']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors, "\n")
for objectives in objectives_list[:2]: print(objectives, "\n")
for rankings in rankings_list[:2]: print(rankings, "\n")
reticulate::repl_python()
