margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
from functions import compare_json
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
differences = compare_json(model_params, best_trial_params)
params_tbl = pd.DataFrame(differences)
params_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)
print(params_tbl)
# Fit the model on the entire dataset
# Initialize the XGBClassifier without early stopping here
# Load the best trial parameters from a JSON file.
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
start = time.time()
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=best_trial_params["max_depth"],
min_child_weight=best_trial_params["min_child_weight"],
gamma=best_trial_params["gamma"],
subsample=best_trial_params["subsample"],
colsample_bytree=best_trial_params["colsample_bytree"],
learning_rate=best_trial_params["learning_rate"],
n_estimators=best_trial_params["n_estimators"],
)
clf.fit(X, y)
end= time.time()
modeling_time = end - start
print(f"\nTraining time: {modeling_time} seconds\n")
# Calculate and print the training accuracy
training_accuracy = clf.score(X, y)
print(f"Training accuracy: {training_accuracy * 100:.2f}%")
# Predict the target for new data
y_pred = clf.predict(X_new)
# Probability estimates
start = time.time()
y_pred_proba = clf.predict_proba(X_new)
end = time.time()
prediction_time = end - start
print(f"\nPrediction time: {prediction_time} seconds\n")
print(f"test_rankings = {np.array(test_rankings)[:6]}, \ny_pred = {y_pred[:6]}, \ny_pred_proba = \n{y_pred_proba[:6]}")
errors = np.abs(y_pred - np.array(test_rankings))
ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)
df = pd.DataFrame({"Ambiguousness": ambiguousness, "Error": errors, "Schedules": test_neighbors, "Objectives": test_objectives}).sort_values(by="Ambiguousness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9),
customdata=df[["Schedules", "Objectives"]],
hovertemplate=
"Ambiguousness: %{x} <br>" +
"Error: %{y} <br>" +
"Schedules: %{customdata[0][0]} / %{customdata[0][1]} <br>" +
"Objectives: %{customdata[1]} <br>"
))
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Ambiguousness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
training_time = round(modeling_time, 4)
conventional_time = round(evaluation_time, 4)
xgboost_time = round(prediction_time, 4)
# Define time values for plotting
time_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds
# Calculate evaluations for method 1
method1_evaluations = np.where(time_values >= training_time, (time_values - training_time) / xgboost_time * 1000, 0)
# Calculate evaluations for method 2
method2_evaluations = time_values / conventional_time * 1000
# Create line chart
fig = go.Figure()
# Add method 1 trace
fig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))
# Add method 2 trace
fig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))
# Update layout
fig.update_layout(
title="Speed comparison between XGBoost ranking model and conventional method",
xaxis_title="Time (seconds)",
yaxis_title="Number of Evaluations",
legend_title="Methods",
template="plotly_white"
)
fig.show()
import time
import math
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.base import clone
import xgboost as xgb
from xgboost.callback import TrainingCallback
import plotly.graph_objects as go
import pickle
import random
from scipy.optimize import minimize
from itertools import combinations
from functions import compute_convolutions
N = 14 # Number of patients
T = 18 # Number of intervals
d = 3 # Length of each interval
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.8 # Weight for the waiting time in objective function
num_schedules = 30000 # Number of schedules to sample
# N = 20 # Number of patients
# T = 18 # Number of intervals
# d = 5 # Length of each interval
# q = 0.20 # Probability of a scheduled patient not showing up
# w = 0.8 # Weight for the waiting time in objective function
# num_schedules = 1000000 # Number of schedules to sample
# Create service time distribution
def generate_weighted_list(T, l, i):
# Initialize an array of T+1 values, starting with zero
values = np.zeros(T + 1)
# Objective function: Sum of squared differences between current weighted average and the desired l
def objective(x):
weighted_avg = np.dot(np.arange(1, T + 1), x) / np.sum(x)
return (weighted_avg - l) ** 2
# Constraint: The sum of the values from index 1 to T must be 1
constraints = ({
'type': 'eq',
'fun': lambda x: np.sum(x) - 1
})
# Bounds: Each value should be between 0 and 1
bounds = [(0, 1)] * T
# Initial guess: Random distribution that sums to 1
initial_guess = np.random.dirichlet(np.ones(T))
# Optimization: Minimize the objective function subject to the sum and bounds constraints
result = minimize(objective, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)
# Set the values in the array (index 0 remains 0)
values[1:] = result.x
# Now we need to reorder the values as per the new requirement
first_part = np.sort(values[1:i+1])  # Sort the first 'i' values in ascending order
second_part = np.sort(values[i+1:])[::-1]  # Sort the remaining 'T-i' values in descending order
# Combine the sorted parts back together
values[1:i+1] = first_part
values[i+1:] = second_part
return values
l = 10
i = 5  # First 5 highest values in ascending order, rest in descending order
s = generate_weighted_list(T, l, i)
print(s)
print("Sum:", np.sum(s[1:]))  # This should be 1
print("Weighted service time:", np.dot(np.arange(1, T + 1), s[1:]))  # This should be close to l
convolutions = compute_convolutions(s, N, q)
from functions import random_combination_with_replacement
start = time.time()
schedules = random_combination_with_replacement(T, N, num_schedules)
print(f"Sampled: {len(schedules)} schedules\n")
h = random.choices(range(len(schedules)), k=7)
print(f"Sampled schedules: {h}")
for i in h:
print(f"Schedule: {schedules[i]}")
end = time.time()
data_prep_time = end - start
print(f"\nProcessing time: {data_prep_time} seconds\n")
def create_neighbors_list(s: list[int], v_star: np.ndarray) -> (list[int], list[int]):
"""
Create a set of pairs of schedules that are from the same neighborhood.
Parameters:
s (list[int]): A list of integers with |s| = T and sum N.
v_star (np.ndarray): Precomputed vectors V* of length T.
Returns:
tuple(list[int], list[int]): A pair of schedules.
"""
T = len(s)
# Precompute binomial coefficients (weights for random.choices)
binom_coeff = [math.comb(T, i) for i in range(1, T)]
# Choose a random value of i with the corresponding probability
i = random.choices(range(1, T), weights=binom_coeff)[0]
# Instead of generating the full list of combinations, sample one directly
j = random.sample(range(T), i)
s_p = s.copy()
for k in j:
s_temp = np.array(s_p) + v_star[k]
s_temp = s_temp.astype(int)
if np.all(s_temp >= 0):
s_p = s_temp.astype(int).tolist()
return s, s_p
def get_v_star(T):
# Create an initial vector 'u' of zeros with length 'T'
u = np.zeros(T)
u[0] = -1
u[-1] = 1
v_star = [u.copy()]
# Generate shifted versions of 'u'
for i in range(T - 1):
u = np.roll(u, 1)
v_star.append(u.copy())
return np.array(v_star)
start = time.time()
v_star = get_v_star(T)
neighbors_list = [create_neighbors_list(schedule, v_star) for schedule in schedules] # This can be done in parellel to improve speed
end = time.time()
for i in h:
original_schedule = neighbors_list[i][0]
neighbor_schedule = neighbors_list[i][1]
difference = [int(x - y) for x, y in zip(neighbors_list[i][0], neighbors_list[i][1])]
print(f"Neighbors\n{original_schedule}\n{neighbor_schedule}\n{difference}")
training_set_feat_time = end - start
print(f"\nProcessing time: {training_set_feat_time} seconds\n")
from functions import calculate_objective_serv_time_lookup
# objectives_schedule_1 = [w * calculate_objective_serv_time_lookup(neighbor[0], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[0], s, d, q)[1] for neighbor in neighbors_list]
# start = time.time()
# objectives_schedule_2 = [w * calculate_objective(neighbor[1], s, d, q)[0] + (1 - w) * calculate_objective(neighbor[1], s, d, q)[1] for neighbor in neighbors_list]
objectives_schedule_1 = [
w * result[0] + (1 - w) * result[1]
for neighbor in neighbors_list
for result in [calculate_objective_serv_time_lookup(neighbor[0], d, q, convolutions)]
]
start = time.time()
objectives_schedule_2 = [
w * result[0] + (1 - w) * result[1]
for neighbor in neighbors_list
for result in [calculate_objective_serv_time_lookup(neighbor[1], d, q, convolutions)]
]
end = time.time()
training_set_lab_time = end - start
objectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]
rankings = np.argmin(objectives, axis=1).tolist()
for i in range(5):
print(f"Objectives: {objectives[i]}, Ranking: {rankings[i]}")
print(f"\nProcessing time: {training_set_lab_time} seconds\n")
# Saving neighbors_list and objectives to a pickle file
file_path_neighbors = f"neighbors_and_objectives-{N}-{T}-{l}.pkl"
with open(file_path_neighbors, 'wb') as f:
pickle.dump({'neighbors_list': neighbors_list, 'objectives': objectives, 'rankings': rankings}, f)
print(f"Data saved successfully to '{file_path_neighbors}'")
# Prepare the dataset
X = []
for neighbors in neighbors_list:
X.append(neighbors[0] + neighbors[1])
X = np.array(X)
y = np.array(rankings)
# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
class CustomCallback(TrainingCallback):
def __init__(self, period=10):
self.period = period
def after_iteration(self, model, epoch, evals_log):
if (epoch + 1) % self.period == 0:
print(f"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}")
return False
def fit_and_score(estimator, X_train, X_test, y_train, y_test):
"""Fit the estimator on the train set and score it on both sets"""
estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0
)
train_score = estimator.score(X_train, y_train)
test_score = estimator.score(X_test, y_test)
return estimator, train_score, test_score
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)
# Initialize the XGBClassifier without early stopping here
# Load the best trial parameters from a JSON file.
with open("model_params.json", "r") as f:
model_params = json.load(f)
# Initialize the EarlyStopping callback with validation dataset
early_stop = xgb.callback.EarlyStopping(
rounds=10, metric_name='logloss', data_name='validation_0', save_best=True
)
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=model_params["max_depth"],
min_child_weight=model_params["min_child_weight"],
gamma=model_params["gamma"],
subsample=model_params["subsample"],
colsample_bytree=model_params["colsample_bytree"],
learning_rate=model_params["learning_rate"],
n_estimators=model_params["n_estimators"],
early_stopping_rounds=9,
#callbacks=[CustomCallback(period=50), early_stop],
callbacks=[CustomCallback(period=50)],
)
print("Params: ")
for key, value in model_params.items():
print(f" {key}: {value}")
start = time.time()
results = []
for train_idx, test_idx in cv.split(X, y):
X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]
est, train_score, test_score = fit_and_score(
clone(clf), X_train, X_test, y_train, y_test
)
results.append((est, train_score, test_score))
end = time.time()
training_time = end - start
print(f"\nTraining time: {training_time} seconds\n")
# Print results
for i, (est, train_score, test_score) in enumerate(results):
print(f"Fold {i+1} - Train Score (Accuracy): {train_score:.4f}, Test Score (Accuracy): {test_score:.4f}")
# Fit the model on the entire dataset
# Initialize the XGBClassifier without early stopping here
start = time.time()
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=model_params["max_depth"],
min_child_weight=model_params["min_child_weight"],
gamma=model_params["gamma"],
subsample=model_params["subsample"],
colsample_bytree=model_params["colsample_bytree"],
learning_rate=model_params["learning_rate"],
n_estimators=model_params["n_estimators"],
)
clf.fit(X, y)
end= time.time()
modeling_time = end - start
# Calculate and print the training accuracy
training_accuracy = clf.score(X, y)
print(f"Training accuracy: {training_accuracy * 100:.2f}%\n")
print(f"\nTraining time: {modeling_time} seconds\n")
num_test_schedules = 1000
test_schedules = random_combination_with_replacement(T, N, num_test_schedules)
test_neighbors = [create_neighbors_list(test_schedule, v_star) for test_schedule in test_schedules] # This can be done in parellel to improve speed
print(f"Sampled: {len(test_schedules)} schedules\n")
test_objectives_schedule_1 = [
w * result[0] + (1 - w) * result[1]
for test_neighbor in test_neighbors
for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, q, convolutions)]
]
# Start time measeurement for the evaluation
start = time.time()
test_objectives_schedule_2 = [
w * result[0] + (1 - w) * result[1]
for test_neighbor in test_neighbors
for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, q, convolutions)]
]
test_rankings = [0 if test_obj < test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]
end = time.time()
evaluation_time = end - start
# Combine the objectives for each pair for later processing
test_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]
print(f"\nEvaluation time: {evaluation_time} seconds\n")
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\n")
input_X = test_neighbors
X_new = []
for test_neighbor in input_X:
X_new.append(test_neighbor[0] + test_neighbor[1])
# Predict the target for new data
y_pred = clf.predict(X_new)
# Probability estimates
start = time.time()
y_pred_proba = clf.predict_proba(X_new)
end = time.time()
prediction_time = end - start
print(f"\nPrediction time: {prediction_time} seconds\n")
print(f"test_rankings = {np.array(test_rankings)[:6]}, \ny_pred = {y_pred[:6]}, \ny_pred_proba = \n{y_pred_proba[:6]}")
from functions import calculate_ambiguousness
errors = np.abs(y_pred - np.array(test_rankings))
ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)
df = pd.DataFrame({"Ambiguousness": ambiguousness, "Error": errors}).sort_values(by="Ambiguousness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9)))
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Ambiguousness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
from functions import compare_json
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
differences = compare_json(model_params, best_trial_params)
params_tbl = pd.DataFrame(differences)
params_tbl.rename(index={'json1_value': 'base parameters', 'json2_value': 'optimized parameters'}, inplace=True)
print(params_tbl)
# Fit the model on the entire dataset
# Initialize the XGBClassifier without early stopping here
# Load the best trial parameters from a JSON file.
with open("best_trial_params.json", "r") as f:
best_trial_params = json.load(f)
start = time.time()
clf = xgb.XGBClassifier(
tree_method="hist",
max_depth=best_trial_params["max_depth"],
min_child_weight=best_trial_params["min_child_weight"],
gamma=best_trial_params["gamma"],
subsample=best_trial_params["subsample"],
colsample_bytree=best_trial_params["colsample_bytree"],
learning_rate=best_trial_params["learning_rate"],
n_estimators=best_trial_params["n_estimators"],
)
clf.fit(X, y)
end= time.time()
modeling_time = end - start
print(f"\nTraining time: {modeling_time} seconds\n")
# Calculate and print the training accuracy
training_accuracy = clf.score(X, y)
print(f"Training accuracy: {training_accuracy * 100:.2f}%")
# Predict the target for new data
y_pred = clf.predict(X_new)
# Probability estimates
start = time.time()
y_pred_proba = clf.predict_proba(X_new)
end = time.time()
prediction_time = end - start
print(f"\nPrediction time: {prediction_time} seconds\n")
print(f"test_rankings = {np.array(test_rankings)[:6]}, \ny_pred = {y_pred[:6]}, \ny_pred_proba = \n{y_pred_proba[:6]}")
errors = np.abs(y_pred - np.array(test_rankings))
ambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)
df = pd.DataFrame({"Ambiguousness": ambiguousness, "Error": errors, "Schedules": test_neighbors, "Objectives": test_objectives}).sort_values(by="Ambiguousness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9),
customdata=df[["Schedules", "Objectives"]],
hovertemplate=
"Ambiguousness: %{x} <br>" +
"Error: %{y} <br>" +
"Schedules: %{customdata[0][0]} / %{customdata[0][1]} <br>" +
"Objectives: %{customdata[1]} <br>"
))
fig.add_trace(go.Scatter(x=df["Ambiguousness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Ambiguousness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Ambiguousness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
training_time = round(modeling_time, 4)
conventional_time = round(evaluation_time, 4)
xgboost_time = round(prediction_time, 4)
# Define time values for plotting
time_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds
# Calculate evaluations for method 1
method1_evaluations = np.where(time_values >= training_time, (time_values - training_time) / xgboost_time * 1000, 0)
# Calculate evaluations for method 2
method2_evaluations = time_values / conventional_time * 1000
# Create line chart
fig = go.Figure()
# Add method 1 trace
fig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))
# Add method 2 trace
fig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))
# Update layout
fig.update_layout(
title="Speed comparison between XGBoost ranking model and conventional method",
xaxis_title="Time (seconds)",
yaxis_title="Number of Evaluations",
legend_title="Methods",
template="plotly_white"
)
fig.show()
from functions import random_combination_with_replacement
start = time.time()
schedules = random_combination_with_replacement(T, N, num_schedules)
print(f"Sampled: {len(schedules):,} schedules\n")
h = random.choices(range(len(schedules)), k=7)
print(f"Sampled schedules: {h}")
for i in h:
print(f"Schedule: {schedules[i]}")
end = time.time()
data_prep_time = end - start
print(f"\nProcessing time: {data_prep_time} seconds\n")
