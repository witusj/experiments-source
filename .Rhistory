import time
import math
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.base import clone
from sklearn.metrics import mean_squared_log_error
import xgboost as xgb
from xgboost.callback import TrainingCallback
import plotly.graph_objects as go
import pickle
N = 12 # Number of patients
T = 18 # Number of intervals
d = 5 # Length of each interval
s = [0.0, 0.27, 0.28, 0.2, 0.15, 0.1] # Service times distribution
q = 0.20 # Probability of a scheduled patient not showing up
w = 0.8 # Weight for the waiting time in objective function
num_schedules = 20000 # Number of schedules to sample
# Load the data from the pickle file
with open('neighbors_and_objectives.pkl', 'rb') as f:
data = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data['neighbors_list']
objectives_list = data['objectives']
rankings_list = data['rankings']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors, "\n")
for objectives in objectives_list[:2]: print(objectives, "\n")
for rankings in rankings_list[:2]: print(rankings, "\n")
# Transform the schedule and objective data into lists of NumPy arrays
X = [item for tup in neighbors_list for item in tup]
y = [item for tup in objectives_list for item in tup]
print(f"Flattened neighbors list: {X[:3]}")
print(f"Flattened objectives list: {y[:3]}")
print(f"Number of schedules: {len(X)}")
# Extract the first schedule and its objective values from the data
# X = np.array([X[0] for X in neighbors_list])
# y = np.array([y[0] for y in objectives_list])
# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#=========================================================================
# XGBoost regression:
# Parameters:
# n_estimators  "Number of gradient boosted trees. Equivalent to number
#                of boosting rounds."
# learning_rate "Boosting learning rate (also known as “eta”)"
# max_depth     "Maximum depth of a tree. Increasing this value will make
#                the model more complex and more likely to overfit."
#=========================================================================
regressor=xgb.XGBRegressor(eval_metric='rmsle')
#=========================================================================
# exhaustively search for the optimal hyperparameters
#=========================================================================
from sklearn.model_selection import GridSearchCV
# set up our search grid
param_grid = {"max_depth":    [4, 5, 7],
"n_estimators": [500, 700, 900],
"learning_rate": [0.05, 0.1, 0.15]}
# try out every combination of the above values
start = time.time()
search = GridSearchCV(regressor, param_grid, cv=5, verbose=3, n_jobs=-1).fit(X_train, y_train)
end = time.time()
hyper_search_time = end - start
print(f'Hyperparameter optimization time: {hyper_search_time}')
print("The best hyperparameters are ",search.best_params_)
regressor=xgb.XGBRegressor(learning_rate = search.best_params_["learning_rate"],
n_estimators  = search.best_params_["n_estimators"],
max_depth     = search.best_params_["max_depth"],
eval_metric='rmsle')
regressor.fit(X_train, y_train)
predictions = regressor.predict(X_test)
# Calculate Mean Absolute Percentage Error (MAPE)
def mean_absolute_percentage_error(y_true, y_pred):
y_true, y_pred = np.array(y_true), np.array(y_pred)
return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
mape = mean_absolute_percentage_error(y_test, predictions)
print(f'MAPE: {mape:.2f}%')
# Create the scatter plot
fig = go.Figure()
fig.add_trace(go.Scatter(
x=y_test,
y=predictions,
mode='markers',
marker=dict(color='blue'),
name='Predictions vs. true values'
))
fig.add_trace(go.Scatter(
x=[0, max(max(y_test), max(predictions))],
y=[0, max(max(y_test), max(predictions))],
mode='lines',
line=dict(color='tomato', dash='dash'),
name='Base line',
))
# Add axis labels and a title
fig.update_layout(
title='Predictions vs. true values',
xaxis_title='True values',
yaxis_title='Predictions',
showlegend=True
)
# Show the plot
fig.show()
from functions import random_combination_with_replacement, create_neighbors_list, calculate_objective
num_test_schedules = 1000
test_schedules = random_combination_with_replacement(T, N, num_test_schedules)
test_neighbors = create_neighbors_list(test_schedules)
print(f"Sampled: {len(test_schedules)} schedules\n")
# Start time measeurement for the evaluation
start = time.time()
test_objectives_schedule_1 = [w * calculate_objective(test_neighbor[0], s, d, q)[0] + (1 - w) * calculate_objective(test_neighbor[0], s, d, q)[1] for test_neighbor in test_neighbors]
test_objectives_schedule_2 = [w * calculate_objective(test_neighbor[1], s, d, q)[0] + (1 - w) * calculate_objective(test_neighbor[1], s, d, q)[1] for test_neighbor in test_neighbors]
end = time.time()
evaluation_time = end - start
test_rankings = [0 if test_obj < test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]
# Combine the objectives for each pair for later processing
test_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]
print(f"\nEvaluation time: {evaluation_time} seconds,\nNumber of evaluated pairs: {len(test_schedules)}\n")
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\n")
def predict_objective(neighbors):
neighbors_array = [np.array(neighbor) for neighbor in neighbors] # Convert schedules to a NumPy array
neighbors_array = np.vstack(neighbors_array)
predictions = regressor.predict(neighbors_array)
return predictions
# Start time measurement for the prediction
start = time.time()
predictions = [regressor.predict(schedule) for schedule in test_schedules]
end = time.time()
prediction_time = end - start
# Calculate the rankings based on the predicted objectives
predictions = [predict_objective(neighbors) for neighbors in test_neighbors]
pred_rankings = [np.argmin(objectives) for objectives in predictions]
print(f"Prediction time: {prediction_time},\nNumber of predicted pairs: {len(predictions)}")
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nPredictions: {predictions[i]}, Ranking: {pred_rankings[i]}\n")
from functions import calculate_opaqueness
errors = np.abs(np.array(test_rankings) - pred_rankings)
accuracy = 1 - errors.mean()
print(f"Accuracy = {accuracy}")
# Calculate the opaqueness of each prediction
normalised_predictions = [prediction / np.sum(prediction) for prediction in predictions]
opaqueness = [calculate_opaqueness(vector) for vector in normalised_predictions]
df = pd.DataFrame({"Opaqueness": opaqueness, "Error": errors}).sort_values(by="Opaqueness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
df.head()
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Opaqueness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9)))
fig.add_trace(go.Scatter(x=df["Opaqueness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Opaqueness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Opaqueness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
test_schedules[:2]
test_schedules[:3]
test_neighbors[:3]
regressor.predict([8, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])
test_neighbors[:3]
regressor.predict(([8, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))
regressor.predict(test_neighbors[:3])
regressor.predict(test_schedules[:3])
regressor.predict(test_schedules)
def predict_objective(neighbors):
neighbors_array = [np.array(neighbor) for neighbor in neighbors] # Convert schedules to a NumPy array
neighbors_array = np.vstack(neighbors_array)
predictions = regressor.predict(neighbors_array)
return predictions
Start time measurement for the prediction
start = time.time()
predictions = regressor.predict(test_schedules)
end = time.time()
prediction_time = end - start
# Calculate the rankings based on the predicted objectives
predictions = [predict_objective(neighbors) for neighbors in test_neighbors]
pred_rankings = [np.argmin(objectives) for objectives in predictions]
print(f"Prediction time: {prediction_time},\nNumber of predicted pairs: {len(predictions)}")
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nPredictions: {predictions[i]}, Ranking: {pred_rankings[i]}\n")
def predict_objective(neighbors):
neighbors_array = [np.array(neighbor) for neighbor in neighbors] # Convert schedules to a NumPy array
neighbors_array = np.vstack(neighbors_array)
predictions = regressor.predict(neighbors_array)
return predictions
# Start time measurement for the prediction
start = time.time()
predictions = regressor.predict(test_schedules)
end = time.time()
prediction_time = end - start
# Calculate the rankings based on the predicted objectives
predictions = [predict_objective(neighbors) for neighbors in test_neighbors]
pred_rankings = [np.argmin(objectives) for objectives in predictions]
print(f"Prediction time: {prediction_time},\nNumber of predicted pairs: {len(predictions)}")
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nPredictions: {predictions[i]}, Ranking: {pred_rankings[i]}\n")
reticulate::repl_python()
