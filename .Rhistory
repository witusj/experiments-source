'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Opaqueness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
training_time = round(training_time, 4)
conventional_time = round(evaluation_time, 4)
xgboost_time = round(prediction_time, 4)
# Define time values for plotting
time_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds
# Calculate evaluations for method 1
method1_evaluations = np.where(time_values >= training_time, (time_values - training_time) / xgboost_time * 1000, 0)
# Calculate evaluations for method 2
method2_evaluations = time_values / conventional_time * 1000
# Create line chart
fig = go.Figure()
# Add method 1 trace
fig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))
# Add method 2 trace
fig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))
# Update layout
fig.update_layout(
title="Speed comparison between XGBoost regressor model and conventional method",
xaxis_title="Time (seconds)",
yaxis_title="Number of Evaluations",
legend_title="Methods",
template="plotly_white"
)
fig.show()
import time
import math
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.base import clone
from sklearn.metrics import mean_squared_log_error
import xgboost as xgb
from xgboost.callback import TrainingCallback
import plotly.graph_objects as go
import pickle
file_path_parameters = f"datasets/parameters_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_parameters, 'rb') as f:
data_params = pickle.load(f)
N = data_params['N'] # Number of patients
T = data_params['T'] # Number of intervals
d = data_params['d'] # Length of each interval
max_s = data_params['max_s'] # Maximum service time
q = data_params['q'] # Probability of a scheduled patient not showing up
w = data_params['w'] # Weight for the waiting time in objective function
l = data_params['l']
num_schedules = data_params['num_schedules'] # Number of schedules to sample
convolutions = data_conv['convolutions']
print(f"The data has following keys: {[key for key in data_sch.keys()]}")
for convolution in list(convolutions.items())[:2]: print(convolution, "\n")
file_path_schedules = f"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_schedules, 'rb') as f:
data_sch = pickle.load(f)
# Extract the variables from the loaded data
neighbors_list = data_sch['neighbors_list']
objectives_list = data_sch['objectives']
rankings_list = data_sch['rankings']
print("Data loaded successfully.\n")
for neigbors in neighbors_list[:2]: print(neigbors, "\n")
for objectives in objectives_list[:2]: print(objectives, "\n")
for rankings in rankings_list[:2]: print(rankings, "\n")
# Transform the schedule and objective data into lists of NumPy arrays
X = [item for tup in neighbors_list for item in tup]
y = [item for tup in objectives_list for item in tup]
print(f"Flattened neighbors list: {X[:3]}")
print(f"Flattened objectives list: {y[:3]}")
print(f"Number of schedules: {len(X)}")
# Ensure that X and y are numpy arrays (convert if needed)
X = np.array(X)
y = np.array(y)
# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Check the shapes of X and y to ensure compatibility
print(f"X shape: {X.shape}, y shape: {y.shape}")
#=========================================================================
# XGBoost regression:
# Parameters:
# n_estimators  "Number of gradient boosted trees. Equivalent to number
#                of boosting rounds."
# learning_rate "Boosting learning rate (also known as “eta”)"
# max_depth     "Maximum depth of a tree. Increasing this value will make
#                the model more complex and more likely to overfit."
#=========================================================================
regressor=xgb.XGBRegressor(eval_metric='rmsle')
#=========================================================================
# exhaustively search for the optimal hyperparameters
#=========================================================================
from sklearn.model_selection import GridSearchCV
# set up our search grid
param_grid = {"max_depth":    [4, 5, 7],
"n_estimators": [500, 700, 900],
"learning_rate": [0.05, 0.1, 0.15]}
# try out every combination of the above values
start = time.time()
search = GridSearchCV(regressor, param_grid, cv=5, verbose=3, n_jobs=-1).fit(X_train, y_train)
end = time.time()
hyper_search_time = end - start
print(f'Hyperparameter optimization time: {hyper_search_time}')
print("The best hyperparameters are ", search.best_params_)
regressor=xgb.XGBRegressor(learning_rate = search.best_params_["learning_rate"],
n_estimators  = search.best_params_["n_estimators"],
max_depth     = search.best_params_["max_depth"],
eval_metric='rmsle')
start = time.time()
regressor.fit(X_train, y_train)
end = time.time()
training_time = end - start
print(f"\nTraining time: {training_time} seconds\n")
regressor.save_model('models/regressor_large_instance.json')
# Make predictions on the test set
predictions = regressor.predict(X_test)
# Calculate Mean Absolute Percentage Error (MAPE)
def mean_absolute_percentage_error(y_true, y_pred):
y_true, y_pred = np.array(y_true), np.array(y_pred)
return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
mape = mean_absolute_percentage_error(y_test, predictions)
print(f'MAPE: {mape:.2f}%')
# Create the scatter plot
fig = go.Figure()
fig.add_trace(go.Scatter(
x=y_test,
y=predictions,
mode='markers',
marker=dict(color='blue'),
name='Predictions vs. true values'
))
fig.add_trace(go.Scatter(
x=[0, max(max(y_test), max(predictions))],
y=[0, max(max(y_test), max(predictions))],
mode='lines',
line=dict(color='tomato', dash='dash'),
name='Base line',
))
# Add axis labels and a title
fig.update_layout(
title='Predictions vs. true values',
xaxis_title='True values',
yaxis_title='Predictions',
showlegend=True
)
# Show the plot
fig.show()
from functions import random_combination_with_replacement, create_neighbors_list, calculate_objective_serv_time_lookup
num_test_schedules = 1000
test_schedules = random_combination_with_replacement(T, N, num_test_schedules)
test_neighbors = [create_neighbors_list(test_schedule) for test_schedule in test_schedules] # This can be done in parellel to improve speed
print(f"Sampled: {len(test_schedules)} schedules\n")
# Start time measeurement for the evaluation
start = time.time()
test_objectives_schedule_1 = [
w * result[0] + (1 - w) * result[1]
for test_neighbor in test_neighbors
for result in [calculate_objective_serv_time_lookup(test_neighbor[0], d, q, convolutions)]
]
end = time.time()
evaluation_time = end - start
print(f"Evaluation time: {evaluation_time} seconds,\nNumber of evaluated schedules: {len(test_schedules)}\n")
test_objectives_schedule_2 = [
w * result[0] + (1 - w) * result[1]
for test_neighbor in test_neighbors
for result in [calculate_objective_serv_time_lookup(test_neighbor[1], d, q, convolutions)]
]
test_rankings = [0 if test_obj < test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]
# Combine the objectives for each pair for later processing
test_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\n")
def predict_objective(neighbors):
neighbors_array = [np.array(neighbor) for neighbor in neighbors] # Convert schedules to a NumPy array
neighbors_array = np.vstack(neighbors_array)
predictions = regressor.predict(neighbors_array)
return predictions
# Start time measurement for the prediction
start = time.time()
predictions = regressor.predict(test_schedules)
end = time.time()
prediction_time = end - start
print(f"Prediction time: {prediction_time},\nNumber of predicted schedules: {len(predictions)}\n")
# Calculate the rankings based on the predicted objectives
predictions = [predict_objective(neighbors) for neighbors in test_neighbors]
pred_rankings = [np.argmin(objectives) for objectives in predictions]
for i in range(6):
print(f"Neighbors: {test_neighbors[i]},\nPredictions: {predictions[i]}, Ranking: {pred_rankings[i]}\n")
from functions import calculate_opaqueness
errors = np.abs(np.array(test_rankings) - pred_rankings)
accuracy = 1 - errors.mean()
print(f"Accuracy = {accuracy}")
# Calculate the opaqueness of each prediction
normalised_predictions = [prediction / np.sum(prediction) for prediction in predictions]
opaqueness = [calculate_opaqueness(vector) for vector in normalised_predictions]
predicted_values_left = [prediction[0] for prediction in predictions]
df = pd.DataFrame({"Opaqueness": opaqueness, "Error": errors, "Predictions": predictions}).sort_values(by="Opaqueness")
df['Cumulative error rate'] = df['Error'].expanding().mean()
# Calculate cumulative accuracy
df['Cumulative accuracy'] = 1 - df['Cumulative error rate']
print(df.head())
# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=df["Opaqueness"], y=df["Error"],
mode="markers",
name="Error",
marker=dict(size=9),
text=[f'{prediction}' for prediction in df["Predictions"]],))
fig.add_trace(go.Scatter(x=df["Opaqueness"], y=df["Cumulative accuracy"],
mode="lines",
name="Cum. accuracy",
line = dict(width = 3, dash = 'dash')))
fig.update_layout(
title={
'text': f"Error vs Opaqueness</br></br><sub>n={num_test_schedules}</sub>",
'y': 0.95,  # Keep the title slightly higher
'x': 0.02,
'xanchor': 'left',
'yanchor': 'top'
},
xaxis_title="Opaqueness",
yaxis_title="Error / Accuracy",
hoverlabel=dict(font=dict(color='white')),
margin=dict(t=70)  # Add more space at the top of the chart
)
fig.show()
training_time = round(training_time, 4)
conventional_time = round(evaluation_time, 4)
xgboost_time = round(prediction_time, 4)
# Define time values for plotting
time_values = np.linspace(0, training_time+0.1, 1000)  # 0 to 2 seconds
# Calculate evaluations for method 1
method1_evaluations = np.where(time_values >= training_time, (time_values - training_time) / xgboost_time * 1000, 0)
# Calculate evaluations for method 2
method2_evaluations = time_values / conventional_time * 1000
# Create line chart
fig = go.Figure()
# Add method 1 trace
fig.add_trace(go.Scatter(x=time_values, y=method1_evaluations, mode='lines', name='Ranking model'))
# Add method 2 trace
fig.add_trace(go.Scatter(x=time_values, y=method2_evaluations, mode='lines', name='Conventional method'))
# Update layout
fig.update_layout(
title="Speed comparison between XGBoost regressor model and conventional method",
xaxis_title="Time (seconds)",
yaxis_title="Number of Evaluations",
legend_title="Methods",
template="plotly_white"
)
fig.show()
import numpy as np
from itertools import chain, combinations
import sys
from math import comb  # Available in Python 3.8 and later
import xgboost as xgb
from functions import calculate_objective
import pickle
file_path_parameters = f"datasets/parameters_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_parameters, 'rb') as f:
data_params = pickle.load(f)
print(f"The data has following keys: {[key for key in data_sch.keys()]}")
N = data_params['N'] # Number of patients
T = data_params['T'] # Number of intervals
d = data_params['d'] # Length of each interval
max_s = data_params['max_s'] # Maximum service time
q = data_params['q'] # Probability of a scheduled patient not showing up
w = data_params['w'] # Weight for the waiting time in objective function
l = data_params['l']
num_schedules = data_params['num_schedules'] # Number of schedules to sample
convolutions = data_conv['convolutions']
for convolution in list(convolutions.items())[:2]: print(convolution, "\n")
# Load the best solution from the training dataset
file_path_schedules = f"datasets/neighbors_and_objectives_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_schedules, 'rb') as f:
data_sch = pickle.load(f)
print(f"The data has following keys: {[key for key in data_sch.keys()]}")
# Step 1: Flatten the objectives into a 1D array
flattened_data = [value for sublist in data_sch['objectives'] for value in sublist]
# Step 2: Find the index of the minimum value
min_index = np.argmin(flattened_data)
# Step 3: Convert that index back to the original 2D structure
row_index = min_index // 2  # Assuming each inner list has 2 values
col_index = min_index % 2
print(f"The minimum objective value is at index [{row_index}][{col_index}].\nThis is schedule: {data_sch['neighbors_list'][row_index][col_index]} with objective value {data_sch['objectives'][row_index][col_index]}.")
# Set the initial schedule to the best solution from the training dataset
initial_schedule = data_sch['neighbors_list'][row_index][col_index]
N = sum(initial_schedule)
T = len(initial_schedule)
T = len(initial_schedule)
def get_v_star(t):
# Create an initial vector 'u' of zeros with length 't'
u = np.zeros(t, dtype=int)
# Set the first element of vector 'u' to -1
u[0] = -1
# Set the last element of vector 'u' to 1
u[-1] = 1
# Initialize the list 'v_star' with the initial vector 'u'
v_star = [u]
# Loop over the length of 'u' minus one times
for i in range(len(u) - 1):
# Append the last element of 'u' to the front of 'u'
u = np.append(u[-1], u)
# Remove the last element of 'u' to maintain the same length
u = np.delete(u, -1)
# Append the updated vector 'u' to the list 'v_star'
v_star.append(u)
# Convert the list of vectors 'v_star' into a NumPy array and return it
return(np.array(v_star))
# Example of function call:
# This will create a 4x4 matrix where each row is a cyclically shifted version of the first row
get_v_star(4)
def powerset(iterable, size=1):
"powerset([1,2,3], 2) --> (1,2) (1,3) (2,3)"
return [[i for i in item] for item in combinations(iterable, size)]
x = initial_schedule
# Generate a matrix 'v_star' using the 'get_v_star' function
v_star = get_v_star(T)
# Generate all possible non-empty subsets (powerset) of the set {0, 1, 2, ..., t-1}
# 'ids' will be a list of tuples, where each tuple is a subset of indices
size = 2
ids = powerset(range(T), size)
len(ids)
ids[:T]
v_star = get_v_star(T)
def get_neighborhood(x, v_star, ids, verbose=False):
x = np.array(x)
p = 50
if verbose:
print(f"Printing every {p}th result")
# Initialize the list 'neighborhood' to store the vectors in the neighborhood of 'x'
neighborhood = []
# Loop over all possible non-empty subsets of indices
for i in range(len(ids)):
# Initialize the vector 'neighbor' to store the sum of vectors in 'v_star' corresponding to the indices in 'ids[i]'
neighbor = np.zeros(len(x), dtype=int)
# Loop over all indices in 'ids[i]'
for j in range(len(ids[i])):
if verbose:
print(f"v_star{[ids[i][j]]}: {v_star[ids[i][j]]}")
# Add the vector in 'v_star' corresponding to the index 'ids[i][j]' to 'neighbor'
neighbor += v_star[ids[i][j]]
# Append the vector 'x' plus 'neighbor' to the list 'neighborhood'
x_n = x + neighbor
if i%p==0:
if verbose:
print(f"x, x', delta:\n{x},\n{x_n},\n{neighbor}\n----------------- ")
neighborhood.append(x_n)
# Convert the list 'neighborhood' into a NumPy array
neighborhood = np.array(neighborhood)
if verbose:
print(f"Size of raw neighborhood: {len(neighborhood)}")
# Create a mask for rows with negative values
mask = ~np.any(neighborhood < 0, axis=1)
# Filter out rows with negative values using the mask
if verbose:
print(f"filtered out: {len(neighborhood)-mask.sum()} schedules with negative values.")
filtered_neighborhood = neighborhood[mask]
if verbose:
print(f"Size of filtered neighborhood: {len(filtered_neighborhood)}")
return filtered_neighborhood
# Example of function call:
# This will generate the neighborhood of the vector 'x' using the vectors in 'v_star' and the indices in 'ids'
test_nh = get_neighborhood(x, v_star, ids)
print(f"All neighborhoods with {size} patients switched:\n x = {np.array(x)}: \n {test_nh}")
from functions import calculate_objective_serv_time_lookup
file_path_convolutions = f"datasets/convolutions_{N}_{T}_{l}.pkl"
# Load the data from the pickle file
with open(file_path_convolutions, 'rb') as f:
data_conv = pickle.load(f)
convolutions = data_conv['convolutions']
def local_search_predicted(x, d, q, convolutions, w, v_star, regressor, size=2):
# Flatten input and initialize the best solution
x_star = np.array(x).flatten()  # Ensure 1D array
x_star_dmatrix = xgb.DMatrix(x_star.reshape(1, -1))
c_star = regressor.predict(x_star_dmatrix)[0]
solutions_list = []
predictions_list = []
objectives_list = []
# Set T as the length of x_star
T = len(x_star)
# Outer loop for t (number of patients switched)
t = 1
while t < size:
print(f'Running local search {t}')
# Generate neighborhood and use a generator to reduce memory usage
ids_gen = powerset(range(T), t)
neighborhood = get_neighborhood(x_star, v_star, ids_gen)
# Create a list to batch process neighbors
neighbors = []
for neighbor in neighborhood:
neighbors.append(neighbor)
# Convert neighborhood to DMatrix for batch prediction
neighbor_dmatrix = xgb.DMatrix(np.array(neighbors))
predicted_costs = regressor.predict(neighbor_dmatrix)
# Flag to track if we find a better solution
found_better_solution = False
# Evaluate neighbors and update x_star and c_star
for i, (neighbor, cost) in enumerate(zip(neighbors, predicted_costs)):
if cost < c_star:
x_star = neighbor
c_star = cost
objectives = calculate_objective_serv_time_lookup(x_star, d, q, convolutions)
objective_value = w * objectives[0] + (1 - w) * objectives[1]
print(f"Found better solution: {x_star}, pred_cost: {c_star}, real_cost: {objective_value}")
# Update lists with the new best solution
solutions_list.append(x_star)
predictions_list.append(c_star)
objectives_list.append(objective_value)
# Set flag to True and break out of inner loop
found_better_solution = True
break
# If we found a better solution, restart outer loop from t = 1
if found_better_solution:
t = 1  # Restart search with t = 1
else:
t += 1  # Move to next t value if no better solution was found
# Return the best solution found
return x_star, c_star, objective_value, solutions_list, predictions_list, objectives_list
def local_search(x, d, q, convolutions, w, v_star, size=2):
# Initialize the best solution found so far 'x_star' to the input vector 'x'
x_star = np.array(x).flatten()  # Keep as 1D array
# Calculate initial objectives and cost
objectives_star = calculate_objective_serv_time_lookup(x_star, d, q, convolutions)
c_star = w * objectives_star[0] + (1 - w) * objectives_star[1]
# Set the value of 'T' to the length of the input vector 'x'
T = len(x_star)
# Outer loop for the number of patients to switch
t = 1
while t < size:
print(f'Running local search {t}')
# Generate the neighborhood of the current best solution 'x_star' with 't' patients switched
ids_gen = powerset(range(T), t)
neighborhood = get_neighborhood(x_star, v_star, ids_gen)
print(f"Switching {t} patient(s). Size of neighborhood: {len(list(ids_gen))}")
# Flag to track if a better solution is found
found_better_solution = False
for neighbor in neighborhood:
# Calculate objectives for the neighbor
objectives = calculate_objective_serv_time_lookup(neighbor, d, q, convolutions)
cost = w * objectives[0] + (1 - w) * objectives[1]
# Compare scalar costs
if cost < c_star:
x_star = neighbor
c_star = cost
print(f"Found better solution: {x_star}, cost: {c_star}")
# Set the flag to restart the outer loop
found_better_solution = True
break  # Break out of the inner loop
# If a better solution was found, restart the search from t = 1
if found_better_solution:
t = 1  # Restart search with t = 1
else:
t += 1  # Move to the next neighborhood size if no better solution was found
# Return the best solution found 'x_star' and its cost
return x_star, c_star
# Example of using the local search algorithm with a regressor model
# Load regressor model
regressor = xgb.Booster()
regressor.load_model("models/regressor_large_instance.json")
test = local_search_predicted(initial_schedule, d, q, convolutions, w, v_star , regressor, T)
print(test[:3])
print(f"Best solution found: {test[0]}, with predicted cost: {test[1]} and real cost: {test[2]}")
import plotly.graph_objects as go
# Data for plotting
steps = list(range(len(test[4])))  # Convert range to list
predicted_values = test[4]
objective_values = test[5]
# Create the figure
fig = go.Figure()
# Add traces
fig.add_trace(go.Scatter(
x=steps,
y=predicted_values,
mode='lines',
name='Predicted Objective Value',
))
fig.add_trace(go.Scatter(
x=steps,
y=objective_values,
mode='lines',
name='True Objective Value',
marker=dict(size=6, symbol='circle')
))
# Add titles and labels
fig.update_layout(
title='Cost of Best Solution at Each Iteration',
xaxis_title='Iteration',
yaxis_title='Cost',
legend_title='Cost Type',
plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
xaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),
yaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),
)
# Show the plot
fig.show()
# Computing optimal solution with real cost
print(f"Initial schedule: {test[0]}")
test_x = local_search(test[0], d, q, convolutions, w, v_star, T)
test_x_pred = np.array(test_x[0]).flatten()  # Keep as 1D array
test_x_pred_dmatrix = xgb.DMatrix(test_x_pred.reshape(1, -1))
test_c_star_pred = regressor.predict(test_x_pred_dmatrix)[0]
print(f"Best solution found: {test_x [0]}, with true cost: {test_x [1]}, and predicted cost: {test_c_star_pred}")
reticulate::repl_python()
