---
title: "Tutorial Fast Fourier Transformations"
author: "AI generated"
jupyter: python3
---

Below is a comprehensive tutorial on using FFT to work with probability distributions—specifically for computing convolutions of probability mass functions (PMFs). This technique is very useful when you want to determine the distribution of the sum of independent random variables.

---

# 1. Introduction

When you have two independent discrete random variables with PMFs \(f\) and \(g\), the probability distribution of their sum is given by the convolution:

$$
(f * g)[n] = \sum_{k} f[k] \, g[n - k]
$$

For example, if you have a PMF representing the waiting time for a service, convolving that PMF with itself gives you the distribution of the total waiting time for two independent services. Direct convolution can be computationally expensive for long distributions, but using the Fast Fourier Transform (FFT) can accelerate this process.

---

# 2. FFT and Convolution

The **Convolution Theorem** states that the Fourier transform of a convolution is the pointwise product of the Fourier transforms. In other words:

$$
\mathcal{F}(f * g) = \mathcal{F}(f) \cdot \mathcal{F}(g)
$$

This means that instead of summing over products (which is \(O(n^2)\) for two sequences of length \(n\)), we can:

1. **Transform both PMFs into the frequency domain using FFT:**  
   When you apply the FFT to a PMF, you convert it from the time (or probability) domain into the frequency domain. The resulting complex numbers represent the amplitude and phase of the frequency components that make up the original PMF.
   
```{python}
import numpy as np
import plotly.graph_objects as go
import plotly.io as pio

# Define a sample PMF (Probability Mass Function)
pmf = np.array([0.1, 0.2, 0.3, 0.25, 0.15])
print("Original PMF:", pmf)

# ---- Step 1: Transform the PMF into the Frequency Domain using FFT ----
# Applying FFT converts the PMF from the time (probability) domain into the frequency domain.
# The result is an array of complex numbers where each element represents a frequency component.
fft_result = np.fft.rfft(pmf)
print("FFT Result:", fft_result)

# ---- Step 2: Extract Amplitude and Phase ----
# The amplitude (or magnitude) of each complex number tells you the contribution (strength) of that frequency component.
amplitude = np.abs(fft_result)
# The phase indicates the shift (in radians) of the corresponding frequency component.
phase = np.angle(fft_result)
print("Amplitude:", amplitude)
print("Phase:", phase)

# ---- Step 3: Visualize the Frequency Domain Representation ----
# We create an interactive Plotly figure to display both the amplitude and phase spectra.
fig = go.Figure()

# Plot Amplitude Spectrum
fig.add_trace(go.Scatter(
    x=np.arange(len(amplitude)),
    y=amplitude,
    mode='markers+lines',
    name='Amplitude Spectrum'
))

# Plot Phase Spectrum
fig.add_trace(go.Scatter(
    x=np.arange(len(phase)),
    y=phase,
    mode='markers+lines',
    name='Phase Spectrum'
))

fig.update_layout(
    title="FFT of PMF: Amplitude and Phase",
    xaxis_title="Frequency Index",
    yaxis_title="Value",
    legend_title="Spectrum"
)

fig.show()

```
   

2. **Multiply them elementwise:**  
   In the frequency domain, convolution becomes simple multiplication. That is, if you have two arrays representing the Fourier transforms of your PMFs, multiplying them elementwise (i.e., each frequency component multiplied by the corresponding component of the other array) yields the Fourier transform of the convolved PMF.

3. **Convert the product back to the time domain using the inverse FFT:**  
   Once you have the product of the two Fourier-transformed arrays, you apply the inverse FFT (iFFT) to convert this product back to the time domain. The result is the convolution of the original PMFs—the distribution of the sum of the two independent random variables.

This approach leverages the efficiency of the FFT, reducing computational complexity from \(O(n^2)\) to approximately \(O(n \log n)\).

---

# 3. Code Example: FFT-Based Convolution for PMFs

Below is a Python example that:
- Defines a PMF for a discrete random variable.
- Uses an FFT-based function to compute the convolution.
- Visualizes the original and convolved PMFs using Plotly.

Make sure to install the required packages if you haven’t already:

```
pip install numpy plotly
```

```{python}
# --- Define a Sample Probability Mass Function (PMF) ---
# For example, a custom PMF representing a discrete outcome (like a service time)
pmf = np.array([0.1, 0.2, 0.3, 0.25, 0.15])
assert np.isclose(np.sum(pmf), 1.0), "PMF must sum to 1."
print("Original PMF:", pmf)

# --- FFT-Based Convolution Function ---
def fft_convolve(a, b):
    """
    Convolve two 1-D arrays (PMFs) using FFT.
    
    Parameters:
        a, b (np.array): Input probability distributions.
        
    Returns:
        np.array: The convolution result, corresponding to the PMF of the sum.
    """
    # The full convolution length for sequences of lengths L and M is L + M - 1.
    n = len(a) + len(b) - 1
    # Zero-pad to the next power of 2 for efficient FFT computation.
    n_fft = 2 ** int(np.ceil(np.log2(n)))
    A = np.fft.rfft(a, n=n_fft)
    B = np.fft.rfft(b, n=n_fft)
    conv_result = np.fft.irfft(A * B, n=n_fft)[:n]
    return conv_result

# --- Compute Convolutions ---
# Distribution of a single random variable (the original PMF)
pmf_single = pmf

# Distribution of the sum of 2 independent variables (e.g., total service time for 2 events)
pmf_two = fft_convolve(pmf, pmf)

# Distribution of the sum of 3 independent variables
pmf_three = fft_convolve(pmf_two, pmf)

# --- Prepare Data for Visualization ---
# Create x-axis values representing possible outcomes (e.g., sum of service times)
x_single = np.arange(len(pmf_single))
x_two = np.arange(len(pmf_two))
x_three = np.arange(len(pmf_three))
```

```{python}
# --- Plot the Distributions using Plotly ---
fig = go.Figure()

fig.add_trace(go.Bar(x=x_single, y=pmf_single, name="Single PMF"))
fig.add_trace(go.Bar(x=x_two, y=pmf_two, name="Sum of 2 Variables"))
fig.add_trace(go.Bar(x=x_three, y=pmf_three, name="Sum of 3 Variables"))

fig.update_layout(
    title="Probability Distributions via Convolution (Using FFT)",
    xaxis_title="Outcome (Sum)",
    yaxis_title="Probability",
    barmode="group"
)

fig.show()
```

---

# 4. Explanation of the Code

### Defining a PMF

In this example, we define a simple PMF:
- **`pmf`**: A NumPy array representing a discrete probability distribution (for example, service times or waiting times).  
- We use an assertion to ensure that the PMF sums to 1.

### FFT-Based Convolution Function

- **`fft_convolve(a, b)`**:
  - **Length Calculation:**  
    Computes the expected length \( n = \text{len}(a) + \text{len}(b) - 1 \) of the convolution.
  - **Zero-Padding:**  
    Pads the arrays to the next power of 2 (stored in `n_fft`) to maximize FFT efficiency.
  - **FFT Computation:**  
    Uses `np.fft.rfft` to compute the FFTs of the padded PMFs.
  - **Elementwise Multiplication:**  
    Multiplies the FFTs elementwise. This step corresponds to the convolution operation in the time (or probability) domain.
  - **Inverse FFT and Trimming:**  
    Applies the inverse FFT with `np.fft.irfft` and slices the result to return only the first \( n \) elements—the actual convolution result.

### Detailed Explanation of the FFT Steps

1. **Transform Both PMFs into the Frequency Domain Using FFT:**  
   Applying the FFT to each PMF converts it from the time (or probability) domain into the frequency domain. In this new domain, the data is represented as a series of complex numbers, each corresponding to a specific frequency component. The magnitude and phase of these complex numbers capture the contribution of each frequency to the overall shape of the PMF.

2. **Multiply Them Elementwise:**  
   In the frequency domain, convolution becomes a simple multiplication of corresponding frequency components. By multiplying the two Fourier-transformed arrays elementwise, you effectively compute the Fourier transform of the convolution of the original PMFs. This step leverages the Convolution Theorem, which states that the Fourier transform of a convolution is equal to the product of the Fourier transforms.

3. **Convert the Product Back to the Time Domain Using the Inverse FFT:**  
   Finally, applying the inverse FFT (iFFT) to the product converts the data back to the time domain. The resulting array represents the convolution of the original PMFs—the distribution of the sum of the independent random variables. Only the first \( n \) elements (where \( n = \text{len}(a) + \text{len}(b) - 1 \)) are kept since that corresponds to the valid convolution result.

### Computing Convolutions

- **`pmf_two`**: Represents the probability distribution for the sum of two independent random variables (each with PMF `pmf`).
- **`pmf_three`**: Represents the distribution for the sum of three independent random variables, computed by convolving the two-variable PMF with the original PMF.

### Visualization with Plotly

- **Bar Charts:**  
  We use Plotly's bar chart functionality to visualize the PMFs:
  - The x-axis represents the possible outcomes (e.g., the total sum).
  - The y-axis shows the probability for each outcome.
- **Grouped Layout:**  
  The distributions for one, two, and three summed variables are plotted together for easy comparison.

---

# 5. Conclusion

This tutorial has shown how FFT can be leveraged to efficiently compute the convolution of probability distributions. This is particularly useful in probability and statistics when you need to find the distribution of a sum of independent random variables.

Key takeaways:
- **FFT for Convolution:**  
  The FFT reduces the computational cost of convolving long PMFs, enabling efficient analysis.
- **Application to PMFs:**  
  Convolving a PMF with itself gives you the distribution of the sum of independent events—a common problem in probability (e.g., total service time, sum of dice rolls).
- **Visualization:**  
  Plotly provides interactive plots that help you visualize and compare the original and convolved distributions.

Feel free to experiment with different PMFs and convolution depths to see how the distribution evolves when summing independent random variables!

<iframe width="560" height="315" src="https://www.youtube.com/embed/nl9TZanwbBk?si=fZpagWLce90Yb2x6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/E8HeD-MUrjY?si=Wl4j47AR_7N02EAA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## Theoretical Background: DFT on a Constrained Dataset  
A multi-dimensional DFT can be applied even if the data points lie on a constrained subset of the full grid. In this case, each vector **X** has components summing to a constant N, meaning the data lives on an $n$-dimensional hyperplane within an $(n+1)$-dimensional space. Essentially, one degree of freedom is lost due to the linear constraint (the dataset is “degenerate” in one dimension, effectively $(n+1)-1 = n$ dimensional) ([Multinomial distribution - Encyclopedia of Mathematics](https://encyclopediaofmath.org/wiki/Multinomial_distribution#:~:text=are%20the%20parameters%20of%20the,A%20multinomial%20distribution%20is%20a)). The multi-D DFT still represents $f(X)$ as a superposition of multidimensional sinusoidal patterns (plane waves) across the domain ([Discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Discrete_Fourier_transform#:~:text=As%20the%20one,broken%20up%20into%20plane%20waves)), but those patterns must respect the sum constraint. In practice, there are two conceptual ways to handle the transform: either *embed* the data in a full $(n+1)$-D array or define the transform intrinsically on the $n$-D constrained surface. In the embedding approach, we place $f(X)$ in an $(N+1)^{(n+1)}$ grid (since each $x_i$ ranges from 0 to N) and set $f=0$ for points that violate $x_0+\cdots+x_n=N$. A standard $(n+1)$-D DFT can then be computed over this extended array. Because $f$ is zero outside the valid hyperplane, the Fourier transform naturally focuses on variations along the allowed subspace. An alternative view is to eliminate one coordinate using $x_n = N - (x_0+\cdots+x_{n-1})$, reducing the problem to $n$ independent coordinates. The DFT can then be formulated on this $n$-dimensional domain, using basis functions that account for the relationship among components. For instance, one can restrict the Fourier basis to those frequency vectors that sum to zero, which are orthogonal to the vector $(1,1,\dots,1)$ that defines the constraint. This effectively embeds the constraint into the basis functions themselves. Notably, research in numerical analysis has developed Fourier methods on simplex domains (the domain of vectors with a fixed sum) – including definitions of Fourier series, DFT and even FFT algorithms on these non-tensor-product domains ([Multivariate Fourier Transform Methods over Simplex and Super-Simplex Domains](https://journal.global-sci.org/intro/article_detail.html?journal=undefined&article_id=8754#:~:text=In%20this%20paper%20we%20propose,The%20relationship%20between)). This theoretical foundation confirms that it’s valid to perform harmonic analysis on a constrained dataset by working in the reduced-dimensional subspace or appropriately accounting for the constraint in the transform. In summary, a multi-D DFT can be applied to $\{X: \sum_i x_i=N\}$ by either zero-padding the data in a full grid or by using a specialized transform basis on the hyperplane, with the understanding that the data effectively resides in a lower-dimensional space. 

## Techniques to Handle the Sum Constraint in the Fourier Framework  
To incorporate the constraint $x_0+\cdots+x_n=N$ directly into Fourier analysis, several techniques can be used:  

- **Dimension Reduction (Eliminate a Variable):** One straightforward approach is to use the constraint to remove one variable from the analysis. For example, set $x_n = N - (x_0+\cdots+x_{n-1})$ and substitute this into the transform. The Fourier basis functions can be written in terms of the first $n$ components, with $x_n$ no longer independent. In the exponent of the DFT’s kernel $e^{-i(k_0x_0+\cdots+k_n x_n)}$, substitute $x_n = N-\sum_{j=0}^{n-1}x_j$. This yields an exponent $e^{-i[(k_0-k_n)x_0 + \cdots + (k_{n-1}-k_n)x_{n-1} + k_n N]}$. The term $e^{-ik_n N}$ is a constant phase factor (since N is fixed), and the important dependence is on the *differences* $h_j = k_j - k_n$ for $j=0,\dots,n-1$. In effect, only $n$ independent frequency parameters (the $h_j$) matter for the variation of $f$ on the constrained domain. One can thus restrict to frequency vectors that satisfy a relation (like $\sum k_i = 0$ or treat one component as reference) so that the basis functions inherently respect the sum constraint. This technique builds the constraint into the Fourier basis: the resulting sinusoids vary only along directions within the hyperplane.  

- **Lagrange Multiplier or Delta Function Enforcement:** Another technique is to enforce the sum constraint via a delta function in the transform integral/sum. In discrete form, one can insert a Kronecker delta $\delta(x_0+\cdots+x_n - N)$ into the DFT formula to ensure only valid $X$ contribute. Equivalently, one may use an integral representation of the delta:  
  $$\delta\Big(\sum_{i=0}^n x_i - N\Big) = \frac{1}{2\pi}\int_{-\pi}^{\pi} e^{i\omega(\sum_i x_i - N)}\,d\omega.$$  
  Including this in the sum for the Fourier transform yields:  
  $$F(k_0,\dots,k_n) = \sum_{X} f(X)\,\delta(\textstyle\sum_i x_i - N)\,e^{-i(k_0x_0+\cdots+k_nx_n)}.$$  
  Using the identity above, the $\omega$-integral can be exchanged with the sum, which effectively links the frequencies $k_i$ through the factor $e^{i\omega \sum_i x_i} = e^{i\omega N}$ times $e^{-i\sum_i k_i x_i}$. Stationary phase in $\omega$ (or simply evaluating the integral) will impose a condition on $k$ (such as $\sum k_i = \omega$) that mirrors the real-space constraint. In simpler terms, this approach introduces an auxiliary frequency (Fourier dual to the sum constraint) to enforce $x_0+\cdots+x_n=N$, coupling the frequency components. While more abstract, this embeds the constraint in the transform mathematically and can lead to analytical simplifications in some cases (e.g. deriving closed-form transforms). A concrete example of this idea appears in probability theory: the **characteristic function** (Fourier transform of the probability mass function) of a multinomial distribution (which lives on $\{x_i\}$ summing to N) factorizes neatly. It is given by $M(t_1,\dots,t_k)= (p_1 e^{it_1} + \cdots + p_k e^{it_k})^N$ ([Multinomial distribution - Encyclopedia of Mathematics](https://encyclopediaofmath.org/wiki/Multinomial_distribution#:~:text=%24%29,a%20multinomial%20distribution%20is)), which inherently uses the sum $N$ in the exponent. This formula comes from treating each trial’s contribution independently and then enforcing that exactly $N$ trials occur – effectively an application of the exponential trick to embed the sum constraint. Such factorized expressions illustrate how a sum constraint can be handled by introducing a conjugate variable (here the inside of the power) that “collects” the contributions of each part while fixing the total.  

- **Specialized Basis on the Simplex:** Instead of using the standard Fourier basis on a cube and then constraining it, one can construct an orthogonal basis of functions defined *on the simplex* (the set of vectors summing to $N$). Researchers have extended Fourier analysis to simplex domains by finding appropriate eigenfunctions or sinusoidal bases that satisfy the boundary conditions of the simplex ([Multivariate Fourier Transform Methods over Simplex and Super-Simplex Domains](https://journal.global-sci.org/intro/article_detail.html?journal=undefined&article_id=8754#:~:text=In%20this%20paper%20we%20propose,The%20relationship%20between)). For instance, there are generalized sine and cosine transforms defined on a simplex (with certain orthogonality properties) that inherently respect the condition $\sum x_i = N$. Using such a basis means the constraint is built in by design – any function expanded in this basis will automatically satisfy the sum constraint (or be zero off the domain). While this approach is more complex, it ensures *validity* in that we never leave the feasible domain during analysis. It connects to the idea of Fourier transform on a finite group or coset: the set $\{X: \sum x_i=N\}$ can be seen as a slice through $\mathbb{Z}^{n+1}$, and one can perform a transform on this slice by considering characters (complex exponentials) that are constant along directions normal to the slice. In practical terms, using a simplex-based Fourier basis might involve precomputing those basis functions (which could be related to e.g. multivariate Jacobi polynomials or other orthogonal polynomials on a simplex) and then projecting $f(X)$ onto them. This is less common than standard FFT approaches, but it directly embeds the constraint into the analysis.  

- **Zero-Padding and Symmetry:** A simpler (if brute-force) method is to include the constraint by symmetry and padding: as mentioned, fill an $(N+1)\times\cdots\times(N+1)$ grid with the known values of $f$ on valid points and zeros elsewhere. The constraint’s effect is then captured by the pattern of zeros. When taking the DFT of this padded array, the fact that $f$ is zero except on the hyperplane means the frequency-domain representation will have certain symmetries. In particular, shifting the input $X$ by a vector that moves along the constraint surface versus off it will produce different contributions. One notable result is that adding a constant to all components $x_i$ (which moves off the hyperplane) has no effect on $f$, so in frequency space this corresponds to certain frequency combinations yielding identical or related spectral coefficients (specifically, a mode where all $k_i$ are equal cannot influence the function’s variation on the hyperplane except as an overall phase). In effect, the Fourier transform will show that only combinations of frequencies that *differ* among components drive variation in $f(X)$. By analyzing those, one inherently is looking at patterns that respect the zero-sum of index shifts. This zero-padding technique is straightforward, though not the most efficient, and it ensures that when you invert the transform, the result will automatically satisfy the constraint (since the inverse FFT will reproduce the same zeros outside the domain).  

Each of these techniques enforces the $x_0+\cdots+x_n=N$ condition in a different way – either by reducing the problem’s dimensionality, adding analytical constraints into the transform, or choosing a basis aligned with the constraint. The goal is to avoid “mixing” valid and invalid states during the Fourier analysis, thereby ensuring that any patterns or reconstructed signals remain within the physically or combinatorially valid domain.

## Practical Implementation Strategies (Python and Numerical Tools)  
Implementing a multi-dimensional DFT for constrained data can be approached with standard FFT libraries or more specialized tools, depending on the data structure:  

- **Embedding in a Full Grid and Using FFT:** If the dataset of $f(X)$ values is available (or can be computed) for *all* combinations satisfying the sum constraint, one practical approach is to embed this data into a full $(n+1)$-dimensional array of size $(N+1)$ in each dimension. Positions corresponding to invalid vectors (where the sum is not $N$) are set to zero. Then, one can apply a regular FFT on this multi-dimensional array. For example, using NumPy in Python:  
  ```python
  import numpy as np
  N = 5   # total sum
  n = 2   # so X has length 3 (n+1=3 dimensions)
  # Initialize full grid array (dimensions N+1 by N+1 by ... (n+1 times))
  shape = (N+1,)*(n+1)
  f_full = np.zeros(shape, dtype=float)
  # Populate f_full for valid combinations x0+x1+...+x_n = N
  for x0 in range(N+1):
      for x1 in range(N+1 - x0):
          x2 = N - x0 - x1
          f_full[x0, x1, x2] = compute_f(x0, x1, x2)  # placeholder for actual f
  # Now perform multi-dimensional FFT on the full array
  F = np.fft.fftn(f_full)
  ```  
  Here `F` will contain the DFT coefficients (complex amplitudes) for each frequency index $(k_0,k_1,k_2)$. One could then analyze `F` to find dominant frequency components, or perform an inverse FFT (`np.fft.ifftn`) on a modified `F` to reconstruct or filter the function. This method leverages highly-optimized FFT algorithms. Its downside is memory and compute cost: the array size is $(N+1)^{(n+1)}$, which can be much larger than the number of valid points. Nonetheless, for moderate $N$ and $n$, this brute-force approach is straightforward and guarantees that the reconstruction will honor the constraint (since we never provided any data outside it, those regions remain zero after inverse transform). It effectively converts the non-uniform problem into a uniform one that FFT can handle ([Non-uniform discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform#:~:text=NUFFTs%20or%20NFFTs%20and%20have,17)) ([Non-uniform discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform#:~:text=approximation.,17)). Note that the FFT assumes the data is periodic at the array boundaries; if the function $f(X)$ does not naturally tile periodically, this embedding implies a periodic extension where $f$ is zero outside the simplex. This could introduce edge effects in frequency space if $f$ has discontinuities at the domain boundary. In practice, if $f$ is smoothly defined only on the simplex, one might mitigate artifacts by windowing or padding beyond N slightly, but that goes deeper into signal processing heuristics.  

- **Non-Uniform Fourier Transform (NUFFT):** If the dataset consists of randomly sampled points (not filling out the entire set of combinations) or if we want to avoid allocating a mostly-zero grid, we can use non-uniform FFT techniques. The *non-uniform discrete Fourier transform* is a generalization of the DFT that handles data on arbitrary sample positions ([Non-uniform discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform#:~:text=In%20applied%20mathematics%2C%20the%20non,3)). In our scenario, the sample positions are the integer vectors $X$ on the hyperplane. Libraries like **FINUFFT** or **PyNUFFT** allow you to input an arbitrary list of sample coordinates and values, and compute the frequency spectrum efficiently (often in $O(M \log M)$ time instead of $O(M^2)$, where $M$ is the number of sample points) ([Non-uniform discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform#:~:text=While%20a%20naive%20application%20of,17)). These algorithms work by clever interpolation and oversampling schemes that map non-grid points to a uniform grid FFT internally ([Non-uniform discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform#:~:text=NUFFTs%20or%20NFFTs%20and%20have,17)). For example, using a NUFFT library, you could provide the list of coordinates $[(x_0,\dots,x_n)]$ (with $\sum x_i=N$ for each) and the corresponding $f(X)$ values, and request the transform at a set of frequency points. This is especially useful if your data is *sparse* (perhaps you only have values at some random sample vectors, not all possible ones) or if $N$ is large so that a full grid is infeasible. Do note that to reconstruct the entire function via inverse NUFFT, you would typically need as many frequency samples as data points (or some interpolation scheme). If you have fewer samples than the full domain, you might treat reconstruction as a regression or interpolation problem: for instance, fit a truncated Fourier series to the known data and then evaluate it. Python’s scientific stack may not have NUFFT in `numpy` directly, but libraries like **SciPy** (with `scipy.fftpack` or `scipy.fft`) assume uniform grids, so one would turn to external packages (FINUFFT, pynfft, or even implement a custom least-squares using `np.fft` on an oversampled grid).  

- **Iterative or Convolution Approaches:** In some cases, the constrained nature of the domain can be exploited algorithmically. For example, if $n+1$ is the number of components, one can construct $f(X)$ by iterative convolution of contributions from each component. This is essentially how one would compute the distribution of a sum of random variables: start with the distribution for $x_0$, then convolve with the distribution of $x_1$, etc., $n+1$ times. If $f(X)$ has a structure that makes it decomposable (say $f(X)$ is some product or convolution of functions of individual $x_i$), then multi-dimensional FFT can be done by repeated 1D FFTs using convolution theorems. However, for a general arbitrary $f$ given as a table, this may not apply. Still, understanding $f$ as, for example, a result of adding $n+1$ independent contributions could hint at using 1D FFT cyclically. A simple practical strategy along these lines: treat one index (say $x_0$) as a “slow” dimension and the sum of the rest as a “fast” dimension. You can loop over $x_0$, take a 1D FFT in the combined $x_1,\dots,x_n$ direction (which actually corresponds to different remaining sums), and then combine results – but this becomes complex and is essentially re-deriving an $n$-D FFT. In most cases, leveraging existing $n$-D FFT implementations via embedding or using a specialized NUFFT is the preferable route.  

- **Reconstruction and Inverse Transform:** Once a forward DFT is obtained (by any method above), reconstructing $f(X)$ means performing an inverse DFT and then extracting the values on the constrained domain. If a full-grid FFT approach was used, this is as simple as calling `ifftn` and reading off the real parts at the integer coordinates (floating-point error aside, they should match the original $f(X)$). If a NUFFT was used, many libraries support an inverse operation as well – or one can set up a system of equations to solve for the inverse. It’s important to emphasize that if $f(X)$ is only known on the hyperplane, any Fourier-based interpolation outside that hyperplane is an extrapolation. Thus, we ensure to evaluate the inverse transform only at points satisfying the constraint (or we enforce through basis selection that the function is zero elsewhere). In Python, after doing the `fftn` as above, one could modify certain frequency components (for example, to filter noise or emphasize certain patterns) and then do `f_back = np.fft.ifftn(F_modified).real`. The array `f_back` will again have zeros (or negligible values) off the hyperplane and the reconstructed $f$ on the hyperplane. If we wanted just the list of reconstructed values, we could then iterate over valid indices (or use a mask) to collect them. In summary, the practical implementation can range from a straightforward NumPy FFT on a padded array to more advanced non-uniform Fourier transforms. Which is appropriate depends on data availability (complete grid vs random samples) and size (small enough for brute-force vs needing an optimized NUFFT).  

## Example Applications and Use Cases  
Applying multi-dimensional Fourier analysis to data with a fixed-sum constraint can be valuable in various contexts:  

- **Compositional Data Analysis:** In fields like geology, chemistry, or economics, one often deals with *compositional data* – vectors of components that sum to a constant total (e.g. proportions summing to 1, or percentages summing to 100%). For instance, a soil sample might be described by percentages of different minerals that always sum to 100%. If we have a function $f(X)$ defined on these compositions (perhaps an index of material strength, or a cost function depending on resource allocation), applying a multi-D DFT can reveal underlying patterns. For example, there might be a periodic oscillation in $f$ whenever one component fraction goes up and another goes down (indicating a trade-off effect). Fourier analysis would capture this as a frequency spike corresponding to that pairwise substitution. By embedding the compositional constraint in the analysis, we ensure that any identified frequency patterns correspond to actual variations in feasible compositions (and not artifacts of moving outside the valid range). This approach could help, say, in finding cyclical patterns in how output quality changes as you vary a recipe of ingredients that must sum to a fixed amount.  

- **Multinomial and Categorical Data Patterns:** Consider an experiment with $n+1$ possible outcomes (categories) that occur $x_0,\dots,x_n$ times, with a fixed total $N$ trials. The space of possible outcome counts is constrained by $\sum x_i = N$. If $f(X)$ represents something like the log-likelihood of observing a particular count vector $X$, or some goodness-of-fit measure, Fourier analysis can be used to study its structure. In fact, as noted earlier, the characteristic function of the multinomial distribution is one such Fourier transform, and it has a simple form $(p_1 e^{it_1}+\cdots+p_{n+1} e^{it_{n+1}})^N$ ([Multinomial distribution - Encyclopedia of Mathematics](https://encyclopediaofmath.org/wiki/Multinomial_distribution#:~:text=%24%29,a%20multinomial%20distribution%20is)). That formula is not just useful for probability theory (deriving moments, correlations, etc.), but it also exemplifies how a constrained sum leads to a **convolutional structure** in the frequency domain: the product of $(p_1 e^{it_1}+\cdots)$ terms $N$ times indicates that each trial contributes independently to frequencies and the total effect is the $N$-fold convolution. In a more general sense, if $f(X)$ is some measured response on outcome count vectors, taking a DFT might highlight if $f$ is largely driven by certain linear combinations of counts. For example, if $f$ mainly varies with the difference $x_0 - x_1$ (holding others fixed), then the Fourier mode corresponding to $(1,-1,0,\dots,0)$ (in the frequency basis of the hyperplane) will show a strong amplitude. Use cases here include analysis of voting data (where votes for candidates must sum to N) to find patterns like swing behavior between two specific parties, or analyzing contingency tables in statistics where one dimension is constrained. Fourier methods can detect interaction patterns in such tables analogous to how spectral analysis finds frequency content in time series.  

- **Reconstructing Expensive Objective Functions:** In optimization or simulation, one might have an objective function $f(x_0,\dots,x_n)$ that is expensive to evaluate, but known to obey a conservation law $x_0+\cdots+x_n=N$ (for instance, distributing a fixed budget across $n+1$ projects and measuring performance). By sampling $f$ at various allocation vectors $X$ (perhaps randomly), one can attempt to reconstruct or approximate $f$ using Fourier techniques. If $f$ is reasonably smooth over the simplex of allocations, it may be approximated by a truncated Fourier series (just as a smooth periodic function can be approximated by a few low-frequency Fourier components). Practically, one could use the sample points and values to fit a model $f(X) \approx \sum_{k} c_k e^{i(k\cdot X)}$ with appropriate constraint on $k$ (such as $\sum k_i=0$ so that $k\cdot X$ really only depends on the internal degrees of freedom). This is essentially a form of response surface modeling. The benefit is that once $f$ is reconstructed (even approximately), one can evaluate it cheaply anywhere on the domain or analyze its behavior by examining the Fourier coefficients $c_k$. The fixed-sum constraint reduces the dimensionality of the problem and can be leveraged to cut down the number of terms needed or to simplify the fitting (since one coordinate is dependent). A use case might be in **project portfolio optimization**: say $x_i$ is investment in project $i$ with total budget $N$ fixed. If $f(X)$ is some return or risk measure, a Fourier analysis could identify, for example, that $f$ has an interaction term between projects 1 and 2 (seen as a high-frequency component along the direction where $x_1$ increases while $x_2$ decreases). This insight could guide the decision maker about trade-offs. Similarly, in engineering, if $X$ represents how one distributes material in different components of a system (with fixed total mass), and $f$ is system reliability, Fourier analysis might expose which redistribution of material causes oscillatory changes in reliability.  

- **Signal or Image Processing with Sum Constraints:** Although less common, there are scenarios in image processing or signal processing where a constraint similar to a fixed sum appears. One example is analyzing patterns in *balanced sequences*. Suppose you have a binary sequence of length N with equal numbers of 0s and 1s (so sum is N/2). This is a 1D sequence version of a sum-constrained set. If we assign $x_0$ = number of 1s and $x_1$ = number of 0s (so $x_0+x_1=N$), $f(x_0,x_1)$ could be some complexity or energy measure of sequences with those counts. A 2D DFT on such data could help identify if there’s a periodic preference in how the 1s and 0s are arranged. More directly, one could look at 2D images where each pixel’s R, G, B values sum to a constant (rare in practice unless a normalization is applied). In any case, whenever the data fundamentally lies on a hyperplane due to a conservation law or normalization, embedding that fact into Fourier analysis ensures that discovered frequency components correspond to actual variations rather than moving outside the feasible set.  

Overall, the approach is beneficial when you suspect that the objective function or data has *structure* that can be exploited. Periodic or oscillatory behavior with respect to changes in one part of the vector balanced by opposite changes in another part will manifest in the Fourier spectrum. By using the multi-dimensional DFT, one can identify such patterns (for example, a strong frequency component might indicate a repeating pattern or symmetry in $f$ along the constraint surface). This can be used for feature extraction, data compression (approximating $f$ with a few Fourier modes), or simply gaining insight into how the function behaves. For probabilistic data (multinomial distributions, contingency tables), Fourier methods connect to characteristic functions and moment-generating functions, providing another analytical tool. For deterministic simulations or measurements, it offers a way to interpolate and detect interactions. In summary, any scenario with a fixed-sum vector (from physics – e.g. momentum or mass conservation – to economics – budget allocation – to computer science – fixed-length codes or schedules) could potentially leverage this Fourier approach to analyze or reconstruct the underlying function more efficiently than naive grid evaluations.

## Computational Efficiency and Limitations  
While Fourier techniques are powerful, applying them to high-dimensional constrained data comes with computational considerations:  

- **Dimensionality and Data Size:** The number of points on the hyperplane grows combinatorially with $N$ and $n$. Specifically, the count of integer solutions to $x_0+\cdots+x_n=N$ (with $x_i\ge0$) is $\binom{N+n}{n}$, which can be very large. Even though the data is effectively $n$-dimensional, an embedding into an $(n+1)$-D array of side length $N+1$ has size $(N+1)^{(n+1)}$, which may be mostly zeros but still consumes memory and processing time. For small $n$ and modest $N$, this is fine (e.g. for $n=2, N=100$, roughly 5151 points embedded in a $101^3 \approx 1\,040,000$ grid – manageable). But if $n$ or $N$ increase, the full-grid FFT becomes infeasible. Using a non-uniform FFT or a custom approach that only processes the $M=\binom{N+n}{n}$ actual points is more efficient in those cases. Note, however, that even $M$ itself can be huge for large $n,N$. The Fourier transform will produce $M$ frequency coefficients to fully reconstruct $f$, so the curse of dimensionality isn’t completely avoided – it’s just shifted to a smaller space when possible.  

- **FFT Complexity:** A standard FFT on an array of size $D$ points is $O(D \log D)$. In our context, $D=(N+1)^{(n+1)}$ if using the padded grid. This can far exceed $M$ (the number of actual data points). Non-uniform FFTs can bring the complexity closer to $O(M \log M)$ ([Non-uniform discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform#:~:text=While%20a%20naive%20application%20of,17)) by not processing the zeros explicitly. Still, $\log M$ can be large, and constant factors in NUFFT algorithms are also significant (due to interpolation steps). If only a subset of frequency coefficients are needed (for example, you only care about low-frequency behavior), one could compute those selectively rather than the full transform. There are algorithms for partial DFT or compressive sensing techniques that recover the most significant Fourier coefficients from fewer computations, which might be applied if $f(X)$ is suspected to be sparse in frequency domain.  

- **Accuracy and Sampling Issues:** If the dataset is only a random sample of the true function (and not values on a full grid), using Fourier to reconstruct the full function is essentially an interpolation problem. Fourier series interpolation requires the function to be well-behaved and sufficiently sampled to avoid aliasing. Random sampling does not guarantee that – there could be “gaps” in frequency space leading to aliasing or ill-conditioning in the reconstruction. One might need to impose smoothness assumptions or regularize the Fourier fit (for instance, truncate high-frequency components) to get a stable reconstruction. In practice, one might use least squares to fit a truncated Fourier series to the sampled data. This is computationally expensive if many basis functions are needed. Additionally, if the samples are noisy or $f$ has high-frequency features, the reconstruction might miss fine details. Ensuring the constraint in the analysis (by using the appropriate basis) at least confines the interpolation to the valid domain, but doesn’t by itself solve the sampling density issue.  

- **Boundary Effects and Periodicity:** The DFT treats the input as periodic. When we artificially embed a simplex-shaped domain into a hyper-rectangular array, we’re assuming that $f(X)$ repeats outside the domain with the pattern given by the array. In our embedding we put zeros, which is a specific type of extension (periodic extension with zeros beyond the simplex). This can introduce high-frequency components if the “true” function on the simplex did not actually go to zero at the boundaries. For example, if $f$ at the edges of the domain (where one $x_i$ is N and the rest 0) is not zero, but we padded zeros just outside that edge, the FFT will interpret that as a sharp drop at the boundary, which translates into many high-frequency components (Gibbs phenomenon). Thus, the frequency spectrum might show artifacts due to the discontinuity at the domain boundary. To mitigate this, one could apply a smooth window that brings $f$ to zero at the boundaries of the domain before embedding (at the cost of distorting $f$ somewhat), or use the intrinsic basis approach (Fourier on the simplex) which better handles boundary conditions. This is a limitation to be aware of: the straightforward FFT approach might over-represent high-frequency content that’s not actually part of $f$, but rather an artifact of the extension.  

- **Numerical Stability:** When performing a multi-dimensional DFT on large datasets, numerical precision can be a concern. Summing up oscillating terms $e^{-i k\cdot X}$ can lead to cancellations. If $f(X)$ varies over a large dynamic range, its Fourier coefficients might be very small and sensitive to floating-point error. Using double precision (standard in NumPy) usually suffices, but very large $N$ could pose issues. The NUFFT algorithms also introduce interpolation error – they typically are approximate to a user-specified tolerance. One should check that increasing the precision or oversampling parameter in a NUFFT does not significantly change results, to ensure the transform is accurate.  

- **Interpretation of Results:** A practical limitation is that interpreting the multi-dimensional frequency content is not always intuitive. In 1D, a peak at frequency $k$ means a sinusoidal pattern of that period. In $n$ dimensions with a constraint, a “frequency vector” $\mathbf{k}$ might be tricky to interpret: it corresponds to a plane wave $e^{i\mathbf{k}\cdot \mathbf{x}}$, which in the constrained space could mean a combination of oscillations in several components at once. For example, a mode where $k_0=1, k_1=-1, k_2=0,...,k_n=0$ (satisfying $1 + (-1) + 0 + \cdots + 0 = 0$) indicates that as $x_0$ increases and $x_1$ decreases (with others fixed), $f$ oscillates. In contrast, a mode where $k_0=k_1=\cdots=k_n$ (not orthogonal to the sum constraint) would purely be an overall phase shift and should not appear if we enforce the constraint (or it appears as a DC component that might relate to shifting the function by a constant). So one has to map frequency domain insights back to the original variables carefully. The constraint complicates this slightly, but once you get used to thinking in terms of one fewer degree of freedom, it becomes manageable.  

- **Scaling with N:** If one tries to increase $N`, the domain size grows and so does the computational load. For very large $N$ (say hundreds or thousands) and moderate dimension, even storing the data or transform may be impossible. In such cases, one might look for theoretical simplifications (for instance, if $f(X)$ has known symmetries or can be factorized). If $f$ is sparse (mostly zero) or has low effective rank, techniques like compressive sensing might reconstruct it from fewer samples. But in the worst case, the method doesn’t circumvent the fact that a general function on the simplex of size $N$ requires $\mathcal{O}(M)$ parameters to describe, which is huge. Therefore, this approach is most powerful when $f$ has some *bandlimited* or smooth nature such that it can be approximated by a relatively small number of Fourier components. If $f$ is essentially random on each point with no smoothness, the Fourier transform will be nearly flat (no strong patterns), and attempting to reconstruct or find structure is futile.  

In conclusion, multi-dimensional DFT techniques can be successfully applied to functions on a constrained-sum domain, but one must carefully embed the constraint to get meaningful results. There are efficient algorithms (NUFFTs) to handle non-uniform grids ([Non-uniform discrete Fourier transform - Wikipedia](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform#:~:text=While%20a%20naive%20application%20of,17)) and even research-grade FFTs on simplices ([Multivariate Fourier Transform Methods over Simplex and Super-Simplex Domains](https://journal.global-sci.org/intro/article_detail.html?journal=undefined&article_id=8754#:~:text=In%20this%20paper%20we%20propose,The%20relationship%20between)), which help with performance. Still, the exponential growth of complexity with dimension and the need for adequate sampling are key limitations. For moderate problem sizes where the structure of $f$ warrants it, the Fourier approach provides a powerful lens to examine periodic interactions and to reconstruct the function efficiently. Just keep in mind the trade-offs in complexity and the assumptions (like periodic extension) inherent in using DFTs when drawing conclusions from the frequency domain.